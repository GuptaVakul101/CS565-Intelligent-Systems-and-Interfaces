{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "170101076-CS565-Assignment-1.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "759ZW42cALus",
        "P7hXWqJuCPwI",
        "OkeAM7l_Bn9C",
        "AQ0vDwtcB_hP",
        "Xba7rv-7B3iJ",
        "lG2VnNYdCFmj",
        "v7gz6tr-2DnY",
        "w5UDSwon2Wpk",
        "iCOxumEprB7i",
        "oiXq7DGsrNXE",
        "z4hSATYTrV5S",
        "eXoMdXaXsMAU",
        "r2nro-l6sefG",
        "ToSQ9h9Vsl-I",
        "IZ4LO9XXs35x",
        "OtO2jU1ttA81",
        "vLeY7bTkJTef",
        "6e2ZLQVfJg1m",
        "rAhT0btRPRAF",
        "62UxEgduvgTk"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "759ZW42cALus"
      },
      "source": [
        "#### **VAKUL GUPTA - 170101076**\n",
        "# **INTELLIGENT SYSTEMS AND INTERFACES - CS565 - ASSIGNMENT 1**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7hXcEU1Wl-0A",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "26cbf86b-7ef6-4512-9e0a-900e5c62886b"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-f-iIj0nA32y"
      },
      "source": [
        "### 1.3.1 - Analysis using existing NLP tools"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bKvht5Zbzy1q"
      },
      "source": [
        "#### 0 - PREREQUISITE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xNP6WgFS2NtS"
      },
      "source": [
        "##### Install Basic Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hOg_yfLFQiNM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 479
        },
        "outputId": "95b02a5c-8434-4c03-d0d0-4ece7a1135b3"
      },
      "source": [
        "!pip install indic-nlp-library\n",
        "from nltk.probability import FreqDist\n",
        "import nltk\n",
        "import codecs\n",
        "import numpy as np\n",
        "from nltk.util import ngrams\n",
        "from matplotlib import pyplot\n",
        "import math\n",
        "nltk.download('punkt')\n",
        "!pip install stanza\n",
        "import stanza\n",
        "stanza.download('hi')\n",
        "from indicnlp.tokenize import sentence_tokenize\n",
        "from nltk.tokenize import TreebankWordTokenizer, WordPunctTokenizer\n",
        "from indicnlp.tokenize import indic_tokenize"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: indic-nlp-library in /usr/local/lib/python3.6/dist-packages (0.71)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from indic-nlp-library) (1.18.5)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from indic-nlp-library) (1.1.2)\n",
            "Requirement already satisfied: morfessor in /usr/local/lib/python3.6/dist-packages (from indic-nlp-library) (2.0.6)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas->indic-nlp-library) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->indic-nlp-library) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.7.3->pandas->indic-nlp-library) (1.15.0)\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "Requirement already satisfied: stanza in /usr/local/lib/python3.6/dist-packages (1.1.1)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.6/dist-packages (from stanza) (3.12.4)\n",
            "Requirement already satisfied: torch>=1.3.0 in /usr/local/lib/python3.6/dist-packages (from stanza) (1.6.0+cu101)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from stanza) (4.41.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from stanza) (2.23.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from stanza) (1.18.5)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.6/dist-packages (from protobuf->stanza) (1.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf->stanza) (50.3.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch>=1.3.0->stanza) (0.16.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->stanza) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->stanza) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->stanza) (2020.6.20)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->stanza) (3.0.4)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/master/resources_1.1.0.json: 122kB [00:00, 13.6MB/s]                    \n",
            "2020-09-30 09:37:47 INFO: Downloading default packages for language: hi (Hindi)...\n",
            "2020-09-30 09:37:48 INFO: File exists: /root/stanza_resources/hi/default.zip.\n",
            "2020-09-30 09:37:51 INFO: Finished downloading models and saved to /root/stanza_resources.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MldUqyeZ2V2a"
      },
      "source": [
        "##### Read Input Files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T5ANdQegCO4M",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "4cf1b447-6ba9-4146-abfc-6448a45c9586"
      },
      "source": [
        "englishText = codecs.open('/content/drive/My Drive/Intelligent_Systems_CS565/Assignment-1/en_wiki.txt', 'r').read()\n",
        "hindiText = codecs.open('/content/drive/My Drive/Intelligent_Systems_CS565/Assignment-1/hi_wiki.txt', 'r').read()\n",
        "filterRatio = 0.05\n",
        "englishFilteredText = englishText[0:int(filterRatio*len(englishText))]\n",
        "hindiFilteredText = hindiText[0:int(filterRatio*len(hindiText))]\n",
        "print((len(englishFilteredText)/len(englishText))*100)\n",
        "print((len(hindiFilteredText)/len(hindiText))*100)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4.999999125378184\n",
            "4.999999876692947\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P7hXWqJuCPwI"
      },
      "source": [
        "#### 1 - Sentence Segmentation and Word Tokenization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OkeAM7l_Bn9C"
      },
      "source": [
        "##### Sentence Segmentation - English"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9lV_r-ei5iml"
      },
      "source": [
        "###### First Method"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IGJvgeESwtjD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "93750354-8b2a-416f-c568-ada659a50abe"
      },
      "source": [
        "englishSentTokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
        "englishSentTokens = englishSentTokenizer.tokenize(englishText)\n",
        "print(len(englishSentTokens))\n",
        "print(englishSentTokens[0:10])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "761582\n",
            "['The word \"atom\" was coined by ancient Greek philosophers.', 'However, these ideas were founded in philosophical and theological reasoning rather than evidence and experimentation.', 'As a result, their views on what atoms look like and how they behave were incorrect.', 'They also could not convince everybody, so atomism was but one of a number of competing theories on the nature of matter.', 'It was not until the 19th century that the idea was embraced and refined by scientists, when the blossoming science of chemistry produced discoveries that only the concept of atoms could explain.', 'In the early 1800s, John Dalton used the concept of atoms to explain why elements always react in ratios of small whole numbers (the law of multiple proportions).', 'For instance, there are two types of tin oxide: one is 88.1% tin and 11.9% oxygen and the other is 78.7% tin and 21.3% oxygen (tin(II) oxide and tin dioxide respectively).', 'This means that 100g of tin will combine either with 13.5g or 27g of oxygen.', '13.5 and 27 form a ratio of 1:2, a ratio of small whole numbers.', 'This common pattern in chemistry suggested to Dalton that elements react in whole number multiples of discrete units—in other words, atoms.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PEET9-x85n-c"
      },
      "source": [
        "###### Second Method"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8rYYvLPKxrGZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "dd3f98ba-9591-4883-ea90-9a2d52cdc197"
      },
      "source": [
        "englishSentTokens = nltk.tokenize.regexp_tokenize(englishText, pattern='(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?)\\s', gaps=True)\n",
        "print(len(englishSentTokens))\n",
        "print(englishSentTokens[0:10])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "750330\n",
            "['The word \"atom\" was coined by ancient Greek philosophers.', 'However, these ideas were founded in philosophical and theological reasoning rather than evidence and experimentation.', 'As a result, their views on what atoms look like and how they behave were incorrect.', 'They also could not convince everybody, so atomism was but one of a number of competing theories on the nature of matter.', 'It was not until the 19th century that the idea was embraced and refined by scientists, when the blossoming science of chemistry produced discoveries that only the concept of atoms could explain.', '\\nIn the early 1800s, John Dalton used the concept of atoms to explain why elements always react in ratios of small whole numbers (the law of multiple proportions).', 'For instance, there are two types of tin oxide: one is 88.1% tin and 11.9% oxygen and the other is 78.7% tin and 21.3% oxygen (tin(II) oxide and tin dioxide respectively).', 'This means that 100g of tin will combine either with 13.5g or 27g of oxygen.', '13.5 and 27 form a ratio of 1:2, a ratio of small whole numbers.', 'This common pattern in chemistry suggested to Dalton that elements react in whole number multiples of discrete units—in other words, atoms.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AQ0vDwtcB_hP"
      },
      "source": [
        "##### Sentence Segmentation - Hindi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5FAzCo18J8IH"
      },
      "source": [
        "###### First Method"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0jQHFTXRVPxm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "9cbdf098-abfc-44d9-802f-8a9720eab0c6"
      },
      "source": [
        "hindiSentTokens = sentence_tokenize.sentence_split(hindiText, lang='hi')\n",
        "print(len(hindiSentTokens))\n",
        "print(hindiSentTokens[0:10])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "348593\n",
            "['मास्टर ऑफ़ हेल्थ एडमिनिस्ट्रेशन या मास्टर ऑफ हेल्थकेयर एडमिनिस्ट्रेशन (एमएचए या एम. एच. ए) स्नातकोत्तर (पोस्ट ग्रेजुएशन) की एक पेशेवर डिग्री है जो स्वास्थ्य प्रशासन के क्षेत्र में दी जाती हैं।', 'यह उन छात्रों को प्रदान की जाती हैं जिन्होंने स्वास्थ्य प्रशासन, अस्पताल प्रबंधन एवं अन्य स्वास्थ्य सेवा संगठनों के क्षेत्र में जरूरी ज्ञान और दक्षता हासिल की हैं।', 'इन पाठ्यक्रमो में परिस्थितियों के अनुसार इनके सरंचना में अंतर हो सकता हैं हालांकि व्यवसायी-शिक्षक मॉडल कार्यक्रम आमतौर पर चिकित्सा, स्वास्थ्य व्यवसायों या संबद्ध स्वास्थ्य के कॉलेजों में पाए जाते हैं, कक्षा-आधारित कार्यक्रम व्यवसाय या सार्वजनिक स्वास्थ्य के कॉलेजों में होते हैं।', 'इस पाठ्यक्रम के अध्ययन के दौरान आम तौर पर विद्यार्थियों को जनसंख्या स्वास्थ्य, स्वास्थ्य देखभाल अर्थशास्त्र, स्वास्थ्य नीति, संगठनात्मक व्यवहार, स्वास्थ्य से जुड़े संगठनों के प्रबंधन, स्वास्थ्य विपणन और संचार, मानव संसाधन प्रबंधन, सूचना प्रणाली प्रबंधन के अध्ययन एवं अन्य क्षेत्रों में व्यावहारिक अनुभव की भी आवश्यकता होती है।', 'यह डिग्री प्रोग्राम स्वास्थ्य विषय के स्नातको को प्रबंधन के मुद्दों की विस्तृत एवं गहन समझ और उन्हें वरिष्ठ प्रबंधन भूमिकाओं के लिए तैयार करने के लिए डिज़ाइन किया गया है।', 'यह अमेरिकी, यूरोपीय, ऑस्ट्रेलियाई, भारतीय और श्रीलंकाई (पोस्ट ग्रेजुएट इंस्टीट्यूट ऑफ मेडिसिन- कोलंबो विश्वविद्यालय) विश्वविद्यालयों के द्वारा प्रदान किया जाता हैं।', 'यह डिग्री प्रोग्राम पारंपरिक रूप से स्वास्थ्य प्रशासन के स्थानीय, राज्य और संघीय स्तर पर एवं साथ ही गैर-लाभकारी क्षेत्र पर केंद्रित होता है।', 'एमएचए कार्यक्रमों का उद्देश्य छात्रों को स्वास्थ्य सेवाओं और सिस्टम क्षेत्रों में वरिष्ठ प्रबंधकीय और नियोजन कार्य के लिए जरूरी ज्ञान प्रदान करना है।', 'कार्यक्रम के उद्देश्य उन स्नातकों को विकसित करना है जो सक्षम सामान्य और वित्तीय प्रबंधकों, सक्षम योजनाकारों, सार्वजनिक स्वास्थ्य के बारे में जानकार और स्वास्थ्य देखभाल प्रणालियों के वित्तपोषण, समाज, कानून और नैतिकता के बारे में जानकार हो।', 'इस पाठ्यक्रम के लिए अधिकांश देशों में, आवेदकों को न्यूनतम चार वर्ष की स्नातक से नीचे की डिग्री की आवश्यकता होती है एवं उन्हें स्वास्थ्य प्रणाली में न्यूनतम पेशेवर अनुभव वांछनीय है।']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5aAJCo3lJ-mA"
      },
      "source": [
        "###### Second Method"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cBwoZIYxckEg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "outputId": "12073e24-6e12-42e7-e3fa-39cd5bb916e8"
      },
      "source": [
        "hindiSentTokenizer = stanza.Pipeline(lang='hi', processors='tokenize')\n",
        "hindiSentTokens = hindiSentTokenizer(hindiFilteredText).sentences\n",
        "hindiSentTokens = [sentence.text for sentence in hindiSentTokens]\n",
        "print(len(hindiSentTokens))\n",
        "print(hindiSentTokens[0:10])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-09-29 15:37:46 INFO: Loading these models for language: hi (Hindi):\n",
            "=======================\n",
            "| Processor | Package |\n",
            "-----------------------\n",
            "| tokenize  | hdtb    |\n",
            "=======================\n",
            "\n",
            "2020-09-29 15:37:46 INFO: Use device: cpu\n",
            "2020-09-29 15:37:46 INFO: Loading: tokenize\n",
            "2020-09-29 15:37:47 INFO: Done loading processors!\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "17928\n",
            "['मास्टर ऑफ़ हेल्थ एडमिनिस्ट्रेशन या मास्टर ऑफ हेल्थकेयर एडमिनिस्ट्रेशन (एमएचए या एम.एच.ए) स्नातकोत्तर (पोस्ट ग्रेजुएशन) की एक पेशेवर डिग्री है जो स्वास्थ्य प्रशासन के क्षेत्र में दी जाती हैं।', 'यह उन छात्रों को प्रदान की जाती हैं जिन्होंने स्वास्थ्य प्रशासन, अस्पताल प्रबंधन एवं अन्य स्वास्थ्य सेवा संगठनों के क्षेत्र में जरूरी ज्ञान और दक्षता हासिल की हैं।', 'इन पाठ्यक्रमो में परिस्थितियों के अनुसार इनके सरंचना में अंतर हो सकता हैं', 'हालांकि व्यवसायी-शिक्षक मॉडल कार्यक्रम आमतौर पर चिकित्सा, स्वास्थ्य व्यवसायों या संबद्ध स्वास्थ्य के कॉलेजों में पाए जाते हैं, कक्षा-आधारित कार्यक्रम व्यवसाय या सार्वजनिक स्वास्थ्य के कॉलेजों में होते हैं।', 'इस पाठ्यक्रम के अध्ययन के दौरान आम तौर पर विद्यार्थियों को जनसंख्या स्वास्थ्य, स्वास्थ्य देखभाल अर्थशास्त्र, स्वास्थ्य नीति, संगठनात्मक व्यवहार, स्वास्थ्य से जुड़े संगठनों के प्रबंधन, स्वास्थ्य विपणन और संचार, मानव संसाधन प्रबंधन, सूचना प्रणाली प्रबंधन के अध्ययन एवं अन्य क्षेत्रों में व्यावहारिक अनुभव की भी आवश्यकता होती है।', 'यह डिग्री प्रोग्राम स्वास्थ्य विषय के स्नातको को प्रबंधन के मुद्दों की विस्तृत एवं गहन समझ और उन्हें वरिष्ठ प्रबंधन भूमिकाओं के लिए तैयार करने के लिए डिज़ाइन किया गया है।', 'यह अमेरिकी, यूरोपीय, ऑस्ट्रेलियाई, भारतीय और श्रीलंकाई (पोस्ट ग्रेजुएट इंस्टीट्यूट ऑफ मेडिसिन- कोलंबो विश्वविद्यालय) विश्वविद्यालयों के द्वारा प्रदान किया जाता हैं।', 'यह डिग्री प्रोग्राम पारंपरिक रूप से स्वास्थ्य प्रशासन के स्थानीय, राज्य और संघीय स्तर पर एवं साथ ही गैर-लाभकारी क्षेत्र पर केंद्रित होता है।', 'एमएचए कार्यक्रमों का उद्देश्य छात्रों को स्वास्थ्य सेवाओं और सिस्टम क्षेत्रों में वरिष्ठ प्रबंधकीय और नियोजन कार्य के लिए जरूरी ज्ञान प्रदान करना है।', 'कार्यक्रम के उद्देश्य उन स्नातकों को विकसित करना है जो सक्षम सामान्य और वित्तीय प्रबंधकों, सक्षम योजनाकारों, सार्वजनिक स्वास्थ्य के बारे में जानकार और स्वास्थ्य देखभाल प्रणालियों के वित्तपोषण, समाज, कानून और नैतिकता के बारे में जानकार हो।']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xba7rv-7B3iJ"
      },
      "source": [
        "##### Word Tokenization - English"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MMxZbMiOLh1X"
      },
      "source": [
        "###### First Method"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WierAfID9H1a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "34da73be-0fd7-49c6-b1ae-03c1a49013af"
      },
      "source": [
        "englishWordTokenizer = TreebankWordTokenizer()\n",
        "englishWordTokens = englishWordTokenizer.tokenize(englishText)\n",
        "print(len(englishWordTokens))\n",
        "print(englishWordTokens[0:100])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "18915300\n",
            "['The', 'word', '``', 'atom', \"''\", 'was', 'coined', 'by', 'ancient', 'Greek', 'philosophers.', 'However', ',', 'these', 'ideas', 'were', 'founded', 'in', 'philosophical', 'and', 'theological', 'reasoning', 'rather', 'than', 'evidence', 'and', 'experimentation.', 'As', 'a', 'result', ',', 'their', 'views', 'on', 'what', 'atoms', 'look', 'like', 'and', 'how', 'they', 'behave', 'were', 'incorrect.', 'They', 'also', 'could', 'not', 'convince', 'everybody', ',', 'so', 'atomism', 'was', 'but', 'one', 'of', 'a', 'number', 'of', 'competing', 'theories', 'on', 'the', 'nature', 'of', 'matter.', 'It', 'was', 'not', 'until', 'the', '19th', 'century', 'that', 'the', 'idea', 'was', 'embraced', 'and', 'refined', 'by', 'scientists', ',', 'when', 'the', 'blossoming', 'science', 'of', 'chemistry', 'produced', 'discoveries', 'that', 'only', 'the', 'concept', 'of', 'atoms', 'could', 'explain.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IS15feK08bZH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "0479cafd-d08c-43a4-d0ff-61a019d0dc09"
      },
      "source": [
        "englishWordTokenizer = WordPunctTokenizer() \n",
        "englishWordTokens = englishWordTokenizer.tokenize(englishText)\n",
        "print(len(englishWordTokens))\n",
        "print(englishWordTokens[0:100])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "20372526\n",
            "['The', 'word', '\"', 'atom', '\"', 'was', 'coined', 'by', 'ancient', 'Greek', 'philosophers', '.', 'However', ',', 'these', 'ideas', 'were', 'founded', 'in', 'philosophical', 'and', 'theological', 'reasoning', 'rather', 'than', 'evidence', 'and', 'experimentation', '.', 'As', 'a', 'result', ',', 'their', 'views', 'on', 'what', 'atoms', 'look', 'like', 'and', 'how', 'they', 'behave', 'were', 'incorrect', '.', 'They', 'also', 'could', 'not', 'convince', 'everybody', ',', 'so', 'atomism', 'was', 'but', 'one', 'of', 'a', 'number', 'of', 'competing', 'theories', 'on', 'the', 'nature', 'of', 'matter', '.', 'It', 'was', 'not', 'until', 'the', '19th', 'century', 'that', 'the', 'idea', 'was', 'embraced', 'and', 'refined', 'by', 'scientists', ',', 'when', 'the', 'blossoming', 'science', 'of', 'chemistry', 'produced', 'discoveries', 'that', 'only', 'the', 'concept']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qxzwxHVQLlDf"
      },
      "source": [
        "###### Second Method"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lG2VnNYdCFmj"
      },
      "source": [
        "##### Word Tokenization - Hindi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qF5SKHlKMZVC"
      },
      "source": [
        "###### First Method"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CWDb-uup-TP7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "e1bb3f26-fa83-45a4-f97a-d5efb2c8ab66"
      },
      "source": [
        "hindiWordTokenizer = stanza.Pipeline(lang='hi', processors='tokenize', tokenize_no_ssplit=True)\n",
        "hindiSentTokens = hindiWordTokenizer(hindiFilteredText).sentences\n",
        "hindiWordTokens = []\n",
        "for i, sentence in enumerate(hindiSentTokens):\n",
        "  for token in sentence.tokens:\n",
        "    hindiWordTokens.append(token.text)\n",
        "print(len(hindiWordTokens))\n",
        "print(hindiWordTokens[0:10])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-09-29 16:04:23 INFO: Loading these models for language: hi (Hindi):\n",
            "=======================\n",
            "| Processor | Package |\n",
            "-----------------------\n",
            "| tokenize  | hdtb    |\n",
            "=======================\n",
            "\n",
            "2020-09-29 16:04:23 INFO: Use device: cpu\n",
            "2020-09-29 16:04:23 INFO: Loading: tokenize\n",
            "2020-09-29 16:04:23 INFO: Done loading processors!\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "420958\n",
            "['मास्टर', 'ऑफ़', 'हेल्थ', 'एडमिनिस्ट्रेशन', 'या', 'मास्टर', 'ऑफ', 'हेल्थकेयर', 'एडमिनिस्ट्रेशन', '(']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VvE6ece8McV-"
      },
      "source": [
        "###### Second Method"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cBONlMO19Xww",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "78c58725-4085-439a-8c9c-06ac53ee292b"
      },
      "source": [
        "hindiWordTokens = indic_tokenize.trivial_tokenize(hindiText)\n",
        "print(len(hindiWordTokens))\n",
        "print(hindiWordTokens[0:100])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "8640033\n",
            "['मास्टर', 'ऑफ़', 'हेल्थ', 'एडमिनिस्ट्रेशन', 'या', 'मास्टर', 'ऑफ', 'हेल्थकेयर', 'एडमिनिस्ट्रेशन', '(', 'एमएचए', 'या', 'एम', '.', 'एच', '.', 'ए', ')', 'स्नातकोत्तर', '(', 'पोस्ट', 'ग्रेजुएशन', ')', 'की', 'एक', 'पेशेवर', 'डिग्री', 'है', 'जो', 'स्वास्थ्य', 'प्रशासन', 'के', 'क्षेत्र', 'में', 'दी', 'जाती', 'हैं', '।', 'यह', 'उन', 'छात्रों', 'को', 'प्रदान', 'की', 'जाती', 'हैं', 'जिन्होंने', 'स्वास्थ्य', 'प्रशासन', ',', 'अस्पताल', 'प्रबंधन', 'एवं', 'अन्य', 'स्वास्थ्य', 'सेवा', 'संगठनों', 'के', 'क्षेत्र', 'में', 'जरूरी', 'ज्ञान', 'और', 'दक्षता', 'हासिल', 'की', 'हैं', '।', 'इन', 'पाठ्यक्रमो', 'में', 'परिस्थितियों', 'के', 'अनुसार', 'इनके', 'सरंचना', 'में', 'अंतर', 'हो', 'सकता', 'हैं', 'हालांकि', 'व्यवसायी', '-', 'शिक्षक', 'मॉडल', 'कार्यक्रम', 'आमतौर', 'पर', 'चिकित्सा', ',', 'स्वास्थ्य', 'व्यवसायों', 'या', 'संबद्ध', 'स्वास्थ्य', 'के', 'कॉलेजों', 'में', 'पाए']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sn4PCsz3h7Va"
      },
      "source": [
        "#### Utility Functions [ Analysing N-Grams and Plotting Frequency Distribution ]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xeVSpbAa0oFF"
      },
      "source": [
        "def extractNGrams(data, num):\n",
        "  n_grams = ngrams(data, num)\n",
        "  return [' '.join(grams) for grams in n_grams]\n",
        "\n",
        "def plotFreqDist(freq_dist, n, lang=\"english\", num_samples=100):\n",
        "    frequencies = [freq_dist[sample] for sample,_ in freq_dist.most_common(num_samples)]\n",
        "    pos = np.arange(num_samples)\n",
        "    width = 1.0\n",
        "    \n",
        "    ngramLabel = \"\"\n",
        "    if n == 1:\n",
        "      ngramLabel = lang + \"Unigram\"\n",
        "    elif n == 2:\n",
        "      ngramLabel = lang + \"Bigram\"\n",
        "    elif n == 3:\n",
        "      ngramLabel = lang + \"Trigram\"\n",
        "\n",
        "    ax = pyplot.axes()\n",
        "    ax.set_xticks(pos + (width / 2))\n",
        "    ax.set_xticklabels(pos)\n",
        "    ax.set_xlabel(ngramLabel + \" Rank\")\n",
        "    ax.set_ylabel(ngramLabel + \" Frequency\")\n",
        "    ax.set_title('Frequency Plot of ' + ngramLabel + ' VS Rank')\n",
        "    ax.grid(True)\n",
        "    pyplot.ylim(0, math.ceil(max(frequencies)/5)*5)\n",
        "    pyplot.bar(pos, frequencies, width, color='r', edgecolor='k')\n",
        "    figure = pyplot.gcf()\n",
        "    figure.savefig(ngramLabel, dpi=figure.dpi)\n",
        "    pyplot.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vtUmu-DozuOP"
      },
      "source": [
        "#### 2 - Unigrams"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QED1isfWAmeX"
      },
      "source": [
        "##### Unigrams - English"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kMKWO8FPz6Ww",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "722ace79-e75e-49f3-f5d9-34993aebdb43"
      },
      "source": [
        "englishUniGrams = extractNGrams(englishWordTokens, 1)\n",
        "print(len(englishUniGrams))\n",
        "print(englishUniGrams[0:100])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "20372526\n",
            "['The', 'word', '\"', 'atom', '\"', 'was', 'coined', 'by', 'ancient', 'Greek', 'philosophers', '.', 'However', ',', 'these', 'ideas', 'were', 'founded', 'in', 'philosophical', 'and', 'theological', 'reasoning', 'rather', 'than', 'evidence', 'and', 'experimentation', '.', 'As', 'a', 'result', ',', 'their', 'views', 'on', 'what', 'atoms', 'look', 'like', 'and', 'how', 'they', 'behave', 'were', 'incorrect', '.', 'They', 'also', 'could', 'not', 'convince', 'everybody', ',', 'so', 'atomism', 'was', 'but', 'one', 'of', 'a', 'number', 'of', 'competing', 'theories', 'on', 'the', 'nature', 'of', 'matter', '.', 'It', 'was', 'not', 'until', 'the', '19th', 'century', 'that', 'the', 'idea', 'was', 'embraced', 'and', 'refined', 'by', 'scientists', ',', 'when', 'the', 'blossoming', 'science', 'of', 'chemistry', 'produced', 'discoveries', 'that', 'only', 'the', 'concept']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e3mHITUqolFI"
      },
      "source": [
        "englishUniFreqDist = nltk.FreqDist(englishUniGrams)\n",
        "plotFreqDist(englishUniFreqDist, 1)\n",
        "print('Unique Count: ', len(englishUniFreqDist))\n",
        "print('5 Most Frequent occurring: ', englishUniFreqDist.most_common(5))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ynlUPCJjAt1c"
      },
      "source": [
        "##### Unigrams - Hindi"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HFKnpvR4238s",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "c7b558df-e614-4152-c4aa-a7c79be89941"
      },
      "source": [
        "hindiUnigrams = extractNGrams(hindiWordTokens, 1)\n",
        "print(len(hindiUnigrams))\n",
        "print(hindiUnigrams[0:100])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "8640033\n",
            "['मास्टर', 'ऑफ़', 'हेल्थ', 'एडमिनिस्ट्रेशन', 'या', 'मास्टर', 'ऑफ', 'हेल्थकेयर', 'एडमिनिस्ट्रेशन', '(', 'एमएचए', 'या', 'एम', '.', 'एच', '.', 'ए', ')', 'स्नातकोत्तर', '(', 'पोस्ट', 'ग्रेजुएशन', ')', 'की', 'एक', 'पेशेवर', 'डिग्री', 'है', 'जो', 'स्वास्थ्य', 'प्रशासन', 'के', 'क्षेत्र', 'में', 'दी', 'जाती', 'हैं', '।', 'यह', 'उन', 'छात्रों', 'को', 'प्रदान', 'की', 'जाती', 'हैं', 'जिन्होंने', 'स्वास्थ्य', 'प्रशासन', ',', 'अस्पताल', 'प्रबंधन', 'एवं', 'अन्य', 'स्वास्थ्य', 'सेवा', 'संगठनों', 'के', 'क्षेत्र', 'में', 'जरूरी', 'ज्ञान', 'और', 'दक्षता', 'हासिल', 'की', 'हैं', '।', 'इन', 'पाठ्यक्रमो', 'में', 'परिस्थितियों', 'के', 'अनुसार', 'इनके', 'सरंचना', 'में', 'अंतर', 'हो', 'सकता', 'हैं', 'हालांकि', 'व्यवसायी', '-', 'शिक्षक', 'मॉडल', 'कार्यक्रम', 'आमतौर', 'पर', 'चिकित्सा', ',', 'स्वास्थ्य', 'व्यवसायों', 'या', 'संबद्ध', 'स्वास्थ्य', 'के', 'कॉलेजों', 'में', 'पाए']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WKcF5A6U54sy",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "0da50174-1f8c-455a-a091-2dd4045a59f6"
      },
      "source": [
        "hindiUniFreqDist = nltk.FreqDist(hindiUnigrams)\n",
        "# plotFreqDist(hindiUniFreqDist, 1, \"hindi\")\n",
        "print('Unique Count: ', len(hindiUniFreqDist))\n",
        "print('5 Most Frequent occurring: ', hindiUniFreqDist.most_common(5))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Unique Count:  322350\n",
            "5 Most Frequent occurring:  [('के', 357833), ('।', 321526), ('में', 268249), (',', 267743), ('है', 232156)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v7gz6tr-2DnY"
      },
      "source": [
        "#### 3 - Bigrams"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kzaz4KqcvlLj"
      },
      "source": [
        "##### Bigrams - English"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XXg35bNS2HNR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "b64cfffa-bb60-4eb8-fbac-e6c1a0e3d255"
      },
      "source": [
        "englishBiGrams = extractNGrams(englishWordTokens, 2)\n",
        "print(len(englishBiGrams))\n",
        "print(englishBiGrams[0:100])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "20372525\n",
            "['The word', 'word \"', '\" atom', 'atom \"', '\" was', 'was coined', 'coined by', 'by ancient', 'ancient Greek', 'Greek philosophers', 'philosophers .', '. However', 'However ,', ', these', 'these ideas', 'ideas were', 'were founded', 'founded in', 'in philosophical', 'philosophical and', 'and theological', 'theological reasoning', 'reasoning rather', 'rather than', 'than evidence', 'evidence and', 'and experimentation', 'experimentation .', '. As', 'As a', 'a result', 'result ,', ', their', 'their views', 'views on', 'on what', 'what atoms', 'atoms look', 'look like', 'like and', 'and how', 'how they', 'they behave', 'behave were', 'were incorrect', 'incorrect .', '. They', 'They also', 'also could', 'could not', 'not convince', 'convince everybody', 'everybody ,', ', so', 'so atomism', 'atomism was', 'was but', 'but one', 'one of', 'of a', 'a number', 'number of', 'of competing', 'competing theories', 'theories on', 'on the', 'the nature', 'nature of', 'of matter', 'matter .', '. It', 'It was', 'was not', 'not until', 'until the', 'the 19th', '19th century', 'century that', 'that the', 'the idea', 'idea was', 'was embraced', 'embraced and', 'and refined', 'refined by', 'by scientists', 'scientists ,', ', when', 'when the', 'the blossoming', 'blossoming science', 'science of', 'of chemistry', 'chemistry produced', 'produced discoveries', 'discoveries that', 'that only', 'only the', 'the concept', 'concept of']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ex15myvEqgYO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "3c85eeed-4485-462a-84c8-4e9ef5665da6"
      },
      "source": [
        "englishBiFreqDist = nltk.FreqDist(englishBiGrams)\n",
        "# plotFreqDist(englishBiFreqDist, 2)\n",
        "print('Unique Count: ', len(englishBiFreqDist))\n",
        "print('5 Most Frequent occurring: ', englishBiFreqDist.most_common(5))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Unique Count:  3977129\n",
            "5 Most Frequent occurring:  [('of the', 179431), ('. The', 140543), (', and', 114438), (\"' s\", 105543), ('in the', 104836)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rOjsoyiXtgs9"
      },
      "source": [
        "##### Bigrams - Hindi"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EcBTq5xBtsAe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "83ba55d5-0a9b-408c-dfac-3dcc4b68aaed"
      },
      "source": [
        "hindiBigrams = extractNGrams(hindiWordTokens, 2)\n",
        "print(len(hindiBigrams))\n",
        "print(hindiBigrams[0:100])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "8640032\n",
            "['मास्टर ऑफ़', 'ऑफ़ हेल्थ', 'हेल्थ एडमिनिस्ट्रेशन', 'एडमिनिस्ट्रेशन या', 'या मास्टर', 'मास्टर ऑफ', 'ऑफ हेल्थकेयर', 'हेल्थकेयर एडमिनिस्ट्रेशन', 'एडमिनिस्ट्रेशन (', '( एमएचए', 'एमएचए या', 'या एम', 'एम .', '. एच', 'एच .', '. ए', 'ए )', ') स्नातकोत्तर', 'स्नातकोत्तर (', '( पोस्ट', 'पोस्ट ग्रेजुएशन', 'ग्रेजुएशन )', ') की', 'की एक', 'एक पेशेवर', 'पेशेवर डिग्री', 'डिग्री है', 'है जो', 'जो स्वास्थ्य', 'स्वास्थ्य प्रशासन', 'प्रशासन के', 'के क्षेत्र', 'क्षेत्र में', 'में दी', 'दी जाती', 'जाती हैं', 'हैं ।', '। यह', 'यह उन', 'उन छात्रों', 'छात्रों को', 'को प्रदान', 'प्रदान की', 'की जाती', 'जाती हैं', 'हैं जिन्होंने', 'जिन्होंने स्वास्थ्य', 'स्वास्थ्य प्रशासन', 'प्रशासन ,', ', अस्पताल', 'अस्पताल प्रबंधन', 'प्रबंधन एवं', 'एवं अन्य', 'अन्य स्वास्थ्य', 'स्वास्थ्य सेवा', 'सेवा संगठनों', 'संगठनों के', 'के क्षेत्र', 'क्षेत्र में', 'में जरूरी', 'जरूरी ज्ञान', 'ज्ञान और', 'और दक्षता', 'दक्षता हासिल', 'हासिल की', 'की हैं', 'हैं ।', '। इन', 'इन पाठ्यक्रमो', 'पाठ्यक्रमो में', 'में परिस्थितियों', 'परिस्थितियों के', 'के अनुसार', 'अनुसार इनके', 'इनके सरंचना', 'सरंचना में', 'में अंतर', 'अंतर हो', 'हो सकता', 'सकता हैं', 'हैं हालांकि', 'हालांकि व्यवसायी', 'व्यवसायी -', '- शिक्षक', 'शिक्षक मॉडल', 'मॉडल कार्यक्रम', 'कार्यक्रम आमतौर', 'आमतौर पर', 'पर चिकित्सा', 'चिकित्सा ,', ', स्वास्थ्य', 'स्वास्थ्य व्यवसायों', 'व्यवसायों या', 'या संबद्ध', 'संबद्ध स्वास्थ्य', 'स्वास्थ्य के', 'के कॉलेजों', 'कॉलेजों में', 'में पाए', 'पाए जाते']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r4KXXIJLttIx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "ca224549-b1b1-4b60-8ab6-2933e6c73ac1"
      },
      "source": [
        "hindiBiFreqDist = nltk.FreqDist(hindiBigrams)\n",
        "# plotFreqDist(hindiBiFreqDist, 2, \"hindi\")\n",
        "print('Unique Count: ', len(hindiBiFreqDist))\n",
        "print('5 Most Frequent occurring: ', hindiBiFreqDist.most_common(5))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Unique Count:  2392979\n",
            "5 Most Frequent occurring:  [('है ।', 125369), ('के लिए', 49250), ('हैं ।', 48556), ('है ,', 32259), ('जाता है', 25250)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w5UDSwon2Wpk"
      },
      "source": [
        "#### 4 - Trigrams"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bZpNe4VUv2Vg"
      },
      "source": [
        "##### Trigrams - English"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tlPsPUm72YV5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "faa195e6-c0f4-4bb4-8872-f779267d5c1f"
      },
      "source": [
        "englishTrigrams = extractNGrams(englishWordTokens, 3)\n",
        "print(len(englishTrigrams))\n",
        "print(englishTrigrams[0:100])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "20372524\n",
            "['The word \"', 'word \" atom', '\" atom \"', 'atom \" was', '\" was coined', 'was coined by', 'coined by ancient', 'by ancient Greek', 'ancient Greek philosophers', 'Greek philosophers .', 'philosophers . However', '. However ,', 'However , these', ', these ideas', 'these ideas were', 'ideas were founded', 'were founded in', 'founded in philosophical', 'in philosophical and', 'philosophical and theological', 'and theological reasoning', 'theological reasoning rather', 'reasoning rather than', 'rather than evidence', 'than evidence and', 'evidence and experimentation', 'and experimentation .', 'experimentation . As', '. As a', 'As a result', 'a result ,', 'result , their', ', their views', 'their views on', 'views on what', 'on what atoms', 'what atoms look', 'atoms look like', 'look like and', 'like and how', 'and how they', 'how they behave', 'they behave were', 'behave were incorrect', 'were incorrect .', 'incorrect . They', '. They also', 'They also could', 'also could not', 'could not convince', 'not convince everybody', 'convince everybody ,', 'everybody , so', ', so atomism', 'so atomism was', 'atomism was but', 'was but one', 'but one of', 'one of a', 'of a number', 'a number of', 'number of competing', 'of competing theories', 'competing theories on', 'theories on the', 'on the nature', 'the nature of', 'nature of matter', 'of matter .', 'matter . It', '. It was', 'It was not', 'was not until', 'not until the', 'until the 19th', 'the 19th century', '19th century that', 'century that the', 'that the idea', 'the idea was', 'idea was embraced', 'was embraced and', 'embraced and refined', 'and refined by', 'refined by scientists', 'by scientists ,', 'scientists , when', ', when the', 'when the blossoming', 'the blossoming science', 'blossoming science of', 'science of chemistry', 'of chemistry produced', 'chemistry produced discoveries', 'produced discoveries that', 'discoveries that only', 'that only the', 'only the concept', 'the concept of', 'concept of atoms']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RhBmn-2fqsra",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "80a2a36c-eb9f-4629-ffa7-bc8ca39166e9"
      },
      "source": [
        "englishTriFreqDist = nltk.FreqDist(englishTrigrams)\n",
        "# plotFreqDist(englishTriFreqDist, 3)\n",
        "print('Unique Count: ', len(englishTriFreqDist))\n",
        "print('5 Most Frequent occurring: ', englishTriFreqDist.most_common(5))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Unique Count:  10878682\n",
            "5 Most Frequent occurring:  [(', and the', 14397), ('. In the', 13244), ('the age of', 11064), ('under the age', 9928), ('the United States', 9467)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t6tlOx-cv68O"
      },
      "source": [
        "##### Trigrams - Hindi"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3rjGlHYpv-55",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "6197380c-69e1-43db-c2cc-54b0e957c783"
      },
      "source": [
        "hindiTrigrams = extractNGrams(hindiWordTokens, 3)\n",
        "print(len(hindiTrigrams))\n",
        "print(hindiTrigrams[0:100])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "8640031\n",
            "['मास्टर ऑफ़ हेल्थ', 'ऑफ़ हेल्थ एडमिनिस्ट्रेशन', 'हेल्थ एडमिनिस्ट्रेशन या', 'एडमिनिस्ट्रेशन या मास्टर', 'या मास्टर ऑफ', 'मास्टर ऑफ हेल्थकेयर', 'ऑफ हेल्थकेयर एडमिनिस्ट्रेशन', 'हेल्थकेयर एडमिनिस्ट्रेशन (', 'एडमिनिस्ट्रेशन ( एमएचए', '( एमएचए या', 'एमएचए या एम', 'या एम .', 'एम . एच', '. एच .', 'एच . ए', '. ए )', 'ए ) स्नातकोत्तर', ') स्नातकोत्तर (', 'स्नातकोत्तर ( पोस्ट', '( पोस्ट ग्रेजुएशन', 'पोस्ट ग्रेजुएशन )', 'ग्रेजुएशन ) की', ') की एक', 'की एक पेशेवर', 'एक पेशेवर डिग्री', 'पेशेवर डिग्री है', 'डिग्री है जो', 'है जो स्वास्थ्य', 'जो स्वास्थ्य प्रशासन', 'स्वास्थ्य प्रशासन के', 'प्रशासन के क्षेत्र', 'के क्षेत्र में', 'क्षेत्र में दी', 'में दी जाती', 'दी जाती हैं', 'जाती हैं ।', 'हैं । यह', '। यह उन', 'यह उन छात्रों', 'उन छात्रों को', 'छात्रों को प्रदान', 'को प्रदान की', 'प्रदान की जाती', 'की जाती हैं', 'जाती हैं जिन्होंने', 'हैं जिन्होंने स्वास्थ्य', 'जिन्होंने स्वास्थ्य प्रशासन', 'स्वास्थ्य प्रशासन ,', 'प्रशासन , अस्पताल', ', अस्पताल प्रबंधन', 'अस्पताल प्रबंधन एवं', 'प्रबंधन एवं अन्य', 'एवं अन्य स्वास्थ्य', 'अन्य स्वास्थ्य सेवा', 'स्वास्थ्य सेवा संगठनों', 'सेवा संगठनों के', 'संगठनों के क्षेत्र', 'के क्षेत्र में', 'क्षेत्र में जरूरी', 'में जरूरी ज्ञान', 'जरूरी ज्ञान और', 'ज्ञान और दक्षता', 'और दक्षता हासिल', 'दक्षता हासिल की', 'हासिल की हैं', 'की हैं ।', 'हैं । इन', '। इन पाठ्यक्रमो', 'इन पाठ्यक्रमो में', 'पाठ्यक्रमो में परिस्थितियों', 'में परिस्थितियों के', 'परिस्थितियों के अनुसार', 'के अनुसार इनके', 'अनुसार इनके सरंचना', 'इनके सरंचना में', 'सरंचना में अंतर', 'में अंतर हो', 'अंतर हो सकता', 'हो सकता हैं', 'सकता हैं हालांकि', 'हैं हालांकि व्यवसायी', 'हालांकि व्यवसायी -', 'व्यवसायी - शिक्षक', '- शिक्षक मॉडल', 'शिक्षक मॉडल कार्यक्रम', 'मॉडल कार्यक्रम आमतौर', 'कार्यक्रम आमतौर पर', 'आमतौर पर चिकित्सा', 'पर चिकित्सा ,', 'चिकित्सा , स्वास्थ्य', ', स्वास्थ्य व्यवसायों', 'स्वास्थ्य व्यवसायों या', 'व्यवसायों या संबद्ध', 'या संबद्ध स्वास्थ्य', 'संबद्ध स्वास्थ्य के', 'स्वास्थ्य के कॉलेजों', 'के कॉलेजों में', 'कॉलेजों में पाए', 'में पाए जाते', 'पाए जाते हैं']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zOXcjkt9v_xv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "786b004d-7545-4962-bd4c-97bd1700c1d3"
      },
      "source": [
        "hindiTriFreqDist = nltk.FreqDist(hindiTrigrams)\n",
        "# plotFreqDist(hindiTriFreqDist, 3, \"hindi\")\n",
        "print('Unique Count: ', len(hindiTriFreqDist))\n",
        "print('5 Most Frequent occurring: ', hindiTriFreqDist.most_common(5))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Unique Count:  5508737\n",
            "5 Most Frequent occurring:  [('के रूप में', 16475), ('जाता है ।', 14139), ('करने के लिए', 7987), ('होता है ।', 7927), ('किया जाता है', 6457)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iCOxumEprB7i"
      },
      "source": [
        "### 1.3.2 - Few Basic Questions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MfhRI-EDzYYL"
      },
      "source": [
        "#### DATA ANALYSIS WITHOUT STEMMING"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oiXq7DGsrNXE"
      },
      "source": [
        "#### 1 - Unigrams required for 90% coverage"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z4hSATYTrV5S"
      },
      "source": [
        "##### English"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gTxun8qBrT9J",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d3e2472e-c1b4-4348-fb42-c31e75474c80"
      },
      "source": [
        "englishUniFreqList = np.array(list(reversed(sorted([val for _, val in englishUniFreqDist.items()]))))\n",
        "uniThresholdCount = np.argmin(englishUniFreqList.cumsum() < englishUniFreqDist.N()*0.9)\n",
        "print('Number of Unigrams required for 90% coverage: ' + str(uniThresholdCount))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of Unigrams required for 90% coverage: 11224\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-KahHuGmz4ne"
      },
      "source": [
        "##### Hindi"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2KJm0lnw0DLR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8c5003fd-45c4-433f-e85b-0a51b4c6924e"
      },
      "source": [
        "hindiUniFreqList = np.array(list(reversed(sorted([val for _, val in hindiUniFreqDist.items()]))))\n",
        "uniThresholdCount = np.argmin(hindiUniFreqList.cumsum() < hindiUniFreqDist.N()*0.9)\n",
        "print('Number of Unigrams required for 90% coverage: ' + str(uniThresholdCount))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of Unigrams required for 90% coverage: 13339\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eXoMdXaXsMAU"
      },
      "source": [
        "#### 2 - Bigrams required for 80% coverage"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dHwha_rGsQq2"
      },
      "source": [
        "##### English"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_9AvyD7nsSUJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f63e6e7e-3fdc-437c-e621-24a970cf23a1"
      },
      "source": [
        "englishBiFreqList = np.array(list(reversed(sorted([val for _, val in englishBiFreqDist.items()]))))\n",
        "biThresholdCount = np.argmin(englishBiFreqList.cumsum() < englishBiFreqDist.N()*0.8)\n",
        "print('Number of Bigrams required for 80% coverage: ' + str(biThresholdCount))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of Bigrams required for 80% coverage: 643572\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gmXNq-6cz_bt"
      },
      "source": [
        "##### Hindi"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iGALKdjZ0I-E",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2410f894-cc40-4453-85a6-83173d0491a2"
      },
      "source": [
        "hindiBiFreqList = np.array(list(reversed(sorted([val for _, val in hindiBiFreqDist.items()]))))\n",
        "biThresholdCount = np.argmin(hindiBiFreqList.cumsum() < hindiBiFreqDist.N()*0.8)\n",
        "print('Number of Bigrams required for 80% coverage: ' + str(biThresholdCount))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of Bigrams required for 80% coverage: 664972\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r2nro-l6sefG"
      },
      "source": [
        "#### 3 - Trigrams required for 70% coverage"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ToSQ9h9Vsl-I"
      },
      "source": [
        "##### English"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "btpmheZYsnZF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "61b5685f-58d9-406f-eba3-71dea426c41f"
      },
      "source": [
        "englishTriFreqList = np.array(list(reversed(sorted([val for _, val in englishTriFreqDist.items()]))))\n",
        "triThresholdCount = np.argmin(englishTriFreqList.cumsum() < englishTriFreqDist.N()*0.7)\n",
        "print('Number of Trigrams required for 70% coverage: ' + str(triThresholdCount))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of Trigrams required for 70% coverage: 4766924\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E3dI1V3I0BEh"
      },
      "source": [
        "##### Hindi"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fgLkDvxE0X1U",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "97d50e82-0511-48b3-abfc-aef4e744277a"
      },
      "source": [
        "hindiTriFreqList = np.array(list(reversed(sorted([val for _, val in hindiTriFreqDist.items()]))))\n",
        "triThresholdCount = np.argmin(hindiTriFreqList.cumsum() < hindiTriFreqDist.N()*0.7)\n",
        "print('Number of Trigrams required for 70% coverage: ' + str(triThresholdCount))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of Trigrams required for 70% coverage: 2916727\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IZ4LO9XXs35x"
      },
      "source": [
        "#### DATA ANALYSIS WITH STEMMING"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tPErLK7eQAzq"
      },
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "ps = PorterStemmer()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YQuEgNmNYi6j"
      },
      "source": [
        "englishWordTokensStemmed = [ps.stem(word) for word in englishWordTokens]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kvnT-_kurODS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "2b8d43a3-6d6d-4b73-a9a5-c596145441b1"
      },
      "source": [
        "def generate_hin_stem_words(word):\n",
        "  suffixes = {\n",
        "    1: [u\"ो\",u\"े\",u\"ू\",u\"ु\",u\"ी\",u\"ि\",u\"ा\"],\n",
        "    2: [u\"कर\",u\"ाओ\",u\"िए\",u\"ाई\",u\"ाए\",u\"ने\",u\"नी\",u\"ना\",u\"ते\",u\"ीं\",u\"ती\",u\"ता\",u\"ाँ\",u\"ां\",u\"ों\",u\"ें\"],\n",
        "    3: [u\"ाकर\",u\"ाइए\",u\"ाईं\",u\"ाया\",u\"ेगी\",u\"ेगा\",u\"ोगी\",u\"ोगे\",u\"ाने\",u\"ाना\",u\"ाते\",u\"ाती\",u\"ाता\",u\"तीं\",u\"ाओं\",u\"ाएं\",u\"ुओं\",u\"ुएं\",u\"ुआं\"],\n",
        "    4: [u\"ाएगी\",u\"ाएगा\",u\"ाओगी\",u\"ाओगे\",u\"एंगी\",u\"ेंगी\",u\"एंगे\",u\"ेंगे\",u\"ूंगी\",u\"ूंगा\",u\"ातीं\",u\"नाओं\",u\"नाएं\",u\"ताओं\",u\"ताएं\",u\"ियाँ\",u\"ियों\",u\"ियां\"],\n",
        "    5: [u\"ाएंगी\",u\"ाएंगे\",u\"ाऊंगी\",u\"ाऊंगा\",u\"ाइयाँ\",u\"ाइयों\",u\"ाइयां\"],\n",
        "  }\n",
        "  \n",
        "  for L in 5, 4, 3, 2, 1:\n",
        "    if len(word) > L + 1:\n",
        "      for suf in suffixes[L]:\n",
        "        if word.endswith(suf):\n",
        "          return word[:-L]\n",
        "  return word\n",
        "\n",
        "print('Number of word tokens as input:', len(hindiWordTokens))\n",
        "hindiWordTokensStemmed = [generate_hin_stem_words(word) for word in hindiWordTokens]\n",
        "print(len(hindiWordTokensStemmed))\n",
        "print('Sample Output:', hindiWordTokensStemmed[0:30])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of word tokens as input: 8640033\n",
            "8640033\n",
            "Sample Output: ['मास्टर', 'ऑफ़', 'हेल्थ', 'एडमिनिस्ट्रेशन', 'या', 'मास्टर', 'ऑफ', 'हेल्थकेयर', 'एडमिनिस्ट्रेशन', '(', 'एमएचए', 'या', 'एम', '.', 'एच', '.', 'ए', ')', 'स्नातकोत्तर', '(', 'पोस्ट', 'ग्रेजुएशन', ')', 'की', 'एक', 'पेशेवर', 'डिग्र', 'है', 'जो', 'स्वास्थ्य']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OtO2jU1ttA81"
      },
      "source": [
        "##### Unigrams required for 90% coverage - English"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DJ09zxKR4ibt",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "238e136f-e81d-460b-aa04-9502c6ef46e3"
      },
      "source": [
        "print(len(englishWordTokensStemmed))\n",
        "englishUniGramsStemmed = extractNGrams(englishWordTokensStemmed, 1)\n",
        "print(len(englishUniGramsStemmed))\n",
        "englishUniFreqDistStemmed = nltk.FreqDist(englishUniGramsStemmed)\n",
        "englishUniFreqListStemmed = np.array(list(reversed(sorted([val for _, val in englishUniFreqDistStemmed.items()]))))\n",
        "uniThresholdCountStemmed = np.argmin(englishUniFreqListStemmed.cumsum() < englishUniFreqDistStemmed.N()*0.9)\n",
        "print('Number of Unigrams required for 90% coverage after stemming: ' + str(uniThresholdCountStemmed))\n",
        "print('Unique Count: ', len(englishUniFreqDistStemmed))\n",
        "print('5 Most Frequent occurring: ', englishUniFreqDistStemmed.most_common(5))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "20372526\n",
            "20372526\n",
            "Number of Unigrams required for 90% coverage after stemming: 4499\n",
            "Unique Count:  200158\n",
            "5 Most Frequent occurring:  [('the', 1270134), (',', 994858), ('.', 862133), ('of', 646927), ('and', 501314)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fATZELnRr9Zn"
      },
      "source": [
        "##### Unigrams required for 90% coverage - Hindi"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HykJGIzwsNT2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "43f1fbbc-8e06-4b60-f425-259bbcc02167"
      },
      "source": [
        "print(len(hindiWordTokensStemmed))\n",
        "hindiUniGramsStemmed = extractNGrams(hindiWordTokensStemmed, 1)\n",
        "print(len(hindiUniGramsStemmed))\n",
        "hindiUniFreqDistStemmed = nltk.FreqDist(hindiUniGramsStemmed)\n",
        "hindiUniFreqListStemmed = np.array(list(reversed(sorted([val for _, val in hindiUniFreqDistStemmed.items()]))))\n",
        "uniThresholdCountStemmed = np.argmin(hindiUniFreqListStemmed.cumsum() < hindiUniFreqDistStemmed.N()*0.9)\n",
        "print('Number of Unigrams required for 90% coverage after stemming: ' + str(uniThresholdCountStemmed))\n",
        "print('Unique Count: ', len(hindiUniFreqDistStemmed))\n",
        "print('5 Most Frequent occurring: ', hindiUniFreqDistStemmed.most_common(5))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "8640033\n",
            "8640033\n",
            "Number of Unigrams required for 90% coverage after stemming: 8033\n",
            "Unique Count:  279722\n",
            "5 Most Frequent occurring:  [('के', 357866), ('।', 321526), ('में', 268249), (',', 267743), ('है', 232300)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WqC8RNaoP1Wb"
      },
      "source": [
        "##### Bigrams required for 80% coverage - English"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yQUuqAYy44Ba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "9c4256c3-83d1-42e8-cae9-22f63351308d"
      },
      "source": [
        "englishBiGramsStemmed = extractNGrams(englishWordTokensStemmed, 2)\n",
        "print(len(englishBiGramsStemmed))\n",
        "englishBiFreqDistStemmed = nltk.FreqDist(englishBiGramsStemmed)\n",
        "englishBiFreqListStemmed = np.array(list(reversed(sorted([val for _, val in englishBiFreqDistStemmed.items()]))))\n",
        "biThresholdCountStemmed = np.argmin(englishBiFreqListStemmed.cumsum() < englishBiFreqDistStemmed.N()*0.8)\n",
        "print('Unique Count: ', len(englishBiFreqDistStemmed))\n",
        "print('5 Most Frequent occurring: ', englishBiFreqDistStemmed.most_common(5))\n",
        "print('Number of Bigrams required for 80% coverage after stemming: ' + str(biThresholdCountStemmed))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "20372525\n",
            "Unique Count:  3184100\n",
            "5 Most Frequent occurring:  [('of the', 179997), ('. the', 140918), (', and', 114466), (\"' s\", 105543), ('in the', 105047)]\n",
            "Number of Bigrams required for 80% coverage after stemming: 365889\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F5cpwmwRsCOa"
      },
      "source": [
        "##### Bigrams required for 80% coverage - Hindi"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gEoQMKyPsrLc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "2e796f45-ad55-4933-96a7-6e178c8906b8"
      },
      "source": [
        "hindiBiGramsStemmed = extractNGrams(hindiWordTokensStemmed, 2)\n",
        "print(len(hindiBiGramsStemmed))\n",
        "hindiBiFreqDistStemmed = nltk.FreqDist(hindiBiGramsStemmed)\n",
        "hindiBiFreqListStemmed = np.array(list(reversed(sorted([val for _, val in hindiBiFreqDistStemmed.items()]))))\n",
        "biThresholdCountStemmed = np.argmin(hindiBiFreqListStemmed.cumsum() < hindiBiFreqDistStemmed.N()*0.8)\n",
        "print('Unique Count: ', len(hindiBiFreqDistStemmed))\n",
        "print('5 Most Frequent occurring: ', hindiBiFreqDistStemmed.most_common(5))\n",
        "print('Number of Bigrams required for 80% coverage after stemming: ' + str(biThresholdCountStemmed))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "8640032\n",
            "Unique Count:  2117887\n",
            "5 Most Frequent occurring:  [('है ।', 125371), ('के लिए', 49252), ('हैं ।', 48556), ('जा है', 32300), ('है ,', 32263)]\n",
            "Number of Bigrams required for 80% coverage after stemming: 493541\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9IOAmXjMP1w5"
      },
      "source": [
        "##### Trigrams required for 70% coverage - English\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ah1Q68Et443U",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "e534d736-1657-4395-de9e-2c424ce0b010"
      },
      "source": [
        "englishTriGramsStemmed = extractNGrams(englishWordTokensStemmed, 3)\n",
        "print(len(englishTriGramsStemmed))\n",
        "englishTriFreqDistStemmed = nltk.FreqDist(englishTriGramsStemmed)\n",
        "englishTriFreqListStemmed = np.array(list(reversed(sorted([val for _, val in englishTriFreqDistStemmed.items()]))))\n",
        "triThresholdCountStemmed = np.argmin(englishTriFreqListStemmed.cumsum() < englishTriFreqDistStemmed.N()*0.7)\n",
        "print('Unique Count: ', len(englishTriFreqDistStemmed))\n",
        "print('5 Most Frequent occurring: ', englishTriFreqDistStemmed.most_common(5))\n",
        "print('Number of Trigrams required for 70% coverage after stemming: ' + str(triThresholdCountStemmed))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "20372524\n",
            "Unique Count:  10145300\n",
            "5 Most Frequent occurring:  [(', and the', 14485), ('. In the', 13255), ('the age of', 12013), ('age of 18', 10081), ('under the age', 9929)]\n",
            "Number of Trigrams required for 70% coverage after stemming: 4033542\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EDPaVZN3sJ6R"
      },
      "source": [
        "##### Trigrams required for 70% coverage - Hindi"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hJ3qO5fYsuH-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "88e47241-a2d0-4a5a-d7ca-0a54ce6781b7"
      },
      "source": [
        "hindiTriGramsStemmed = extractNGrams(hindiWordTokensStemmed, 3)\n",
        "print(len(hindiTriGramsStemmed))\n",
        "hindiTriFreqDistStemmed = nltk.FreqDist(hindiTriGramsStemmed)\n",
        "hindiTriFreqListStemmed = np.array(list(reversed(sorted([val for _, val in hindiTriFreqDistStemmed.items()]))))\n",
        "triThresholdCountStemmed = np.argmin(hindiTriFreqListStemmed.cumsum() < hindiTriFreqDistStemmed.N()*0.7)\n",
        "print('Unique Count: ', len(hindiTriFreqDistStemmed))\n",
        "print('5 Most Frequent occurring: ', hindiTriFreqDistStemmed.most_common(5))\n",
        "print('Number of Trigrams required for 70% coverage after stemming: ' + str(triThresholdCountStemmed))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "8640031\n",
            "Unique Count:  5242642\n",
            "5 Most Frequent occurring:  [('जा है ।', 18479), ('के रूप में', 16488), ('हो है ।', 12505), ('कर के लिए', 8091), ('है । यह', 7876)]\n",
            "Number of Trigrams required for 70% coverage after stemming: 2650632\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UwC0BL2ZnJ45"
      },
      "source": [
        "### 1.3.3 - Writing some of your basic codes and comparing with results obtained using tools"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0iBoYKbKzs95"
      },
      "source": [
        "#### 1 - Analysis after implementing Heuristics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1NXrJt7t0pdd"
      },
      "source": [
        "##### Utility fucntion for implementing Heuristics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n9lZtxqmzw7n",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "30e2d89b-4f9c-4908-ef4b-02567f7f24a4"
      },
      "source": [
        "def getTokensAfterHeuristics(text):\n",
        "    pattern = r'''(?x)          # set flag to allow verbose regexps\n",
        "        (?:[A-Z]\\.)+        # abbreviations, e.g. U.S.A.\n",
        "      | (?:\\s\\w\\w\\.)+       # Dr. Mr. Ms.\n",
        "      | \\w+(?:-\\w+)*        # words with optional internal hyphens\n",
        "      | \\$?\\d+(?:\\.\\d+)?%?  # currency and percentages, e.g. $12.40, 82%\n",
        "      | \\.\\.\\.              # ellipsis\n",
        "      | [][.,;\"'?():_`-]    # these are separate tokens; includes ], [\n",
        "    '''\n",
        "    \n",
        "    sentences = nltk.tokenize.RegexpTokenizer(pattern).tokenize(text)\n",
        "    tokens = []\n",
        "    englishTokenizer = TreebankWordTokenizer()\n",
        "\n",
        "    for sentence in sentences:\n",
        "        words = englishTokenizer.tokenize(sentence)\n",
        "        tokens += words\n",
        "    return tokens\n",
        "\n",
        "def ignoreUpperCase(tokens):\n",
        "    tokens = [t.lower() for t in tokens]\n",
        "    return tokens\n",
        "\n",
        "print(\"Tokens count Without Heuristics: \", len(englishWordTokens))\n",
        "englishTokensHeur = getTokensAfterHeuristics(englishText)\n",
        "englishTokensHeur = ignoreUpperCase(englishTokensHeur)\n",
        "print(\"Tokens count With Heuristics: \", len(englishTokensHeur))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tokens count Without Heuristics:  20372526\n",
            "Tokens count With Heuristics:  20065018\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gTuHHOHDZzZM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c1b3c4e4-e588-4048-f71b-4cead3269c8d"
      },
      "source": [
        "def hindiHeuristics(text):\n",
        "  sentences = text.split(u\"।\")\n",
        "  \n",
        "  all_words = []\n",
        "  for sentence in sentences:\n",
        "    words = nltk.word_tokenize(sentence)\n",
        "    all_words += words\n",
        "  return all_words\n",
        "# print(\"Tokens count Without Heuristics: \", len(hindiWordTokens))\n",
        "hindiTokensHeur = hindiHeuristics(hindiText)\n",
        "print(\"Tokens count With Heuristics: \", len(hindiTokensHeur))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tokens count With Heuristics:  8158694\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DG8grYymz9zD"
      },
      "source": [
        "##### DATA ANALYSIS WITHOUT STEMMING"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8-FWdAcz01vm"
      },
      "source": [
        "###### Unigrams required for 90% coverage\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yvkGBlpgV_bK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "f723e366-305f-4f8c-c51e-e011d8774bae"
      },
      "source": [
        "print(len(englishTokensHeur))\n",
        "englishUniGramsHeur = extractNGrams(englishTokensHeur, 1)\n",
        "print(len(englishUniGramsHeur))\n",
        "englishUniFreqDistHeur = nltk.FreqDist(englishUniGramsHeur)\n",
        "englishUniFreqListHeur = np.array(list(reversed(sorted([val for _, val in englishUniFreqDistHeur.items()]))))\n",
        "uniThresholdCountHeur = np.argmin(englishUniFreqListHeur.cumsum() < englishUniFreqDistHeur.N()*0.9)\n",
        "print('Number of Unigrams required for 90% coverage: ' + str(uniThresholdCountHeur))\n",
        "print('Unique Count: ', len(englishUniFreqDistHeur))\n",
        "print('5 Most Frequent occurring: ', englishUniFreqDistHeur.most_common(5))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "20065018\n",
            "20065018\n",
            "Number of Unigrams required for 90% coverage: 9297\n",
            "Unique Count:  297153\n",
            "5 Most Frequent occurring:  [('the', 1269619), (',', 1080241), ('.', 940699), ('of', 647349), ('and', 500543)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hokoFIXe03TC"
      },
      "source": [
        "###### Bigrams required for 80% coverage"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T2-R6hFoWwJj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "0bb53b9e-f2db-4498-b185-f1e2624eee2b"
      },
      "source": [
        "print(len(englishTokensHeur))\n",
        "englishBiGramsHeur = extractNGrams(englishTokensHeur, 2)\n",
        "print(len(englishBiGramsHeur))\n",
        "englishBiFreqDistHeur = nltk.FreqDist(englishBiGramsHeur)\n",
        "englishBiFreqListHeur = np.array(list(reversed(sorted([val for _, val in englishBiFreqDistHeur.items()]))))\n",
        "BiThresholdCountHeur = np.argmin(englishBiFreqListHeur.cumsum() < englishBiFreqDistHeur.N()*0.8)\n",
        "print('Number of Bigrams required for 80% coverage: ' + str(BiThresholdCountHeur))\n",
        "print('Unque Count: ', len(englishBiFreqDistHeur))\n",
        "print('5 Most Frequent occurring: ', englishBiFreqDistHeur.most_common(5))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "20065018\n",
            "20065017\n",
            "Number of Bigrams required for 80% coverage: 521365\n",
            "Unque Count:  3687877\n",
            "5 Most Frequent occurring:  [('of the', 180429), ('. the', 155682), (', and', 123743), ('in the', 120260), (\"' s\", 106041)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZodTy0Qc051B"
      },
      "source": [
        "###### Trigrams required for 70% coverage"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aOocfh-YWxOV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "5a39219b-bd7b-4dc6-fa5d-3f1663b399d6"
      },
      "source": [
        "print(len(englishTokensHeur))\n",
        "englishTriGramsHeur = extractNGrams(englishTokensHeur, 3)\n",
        "print(len(englishTriGramsHeur))\n",
        "englishTriFreqDistHeur = nltk.FreqDist(englishTriGramsHeur)\n",
        "englishTriFreqListHeur = np.array(list(reversed(sorted([val for _, val in englishTriFreqDistHeur.items()]))))\n",
        "TriThresholdCountHeur = np.argmin(englishTriFreqListHeur.cumsum() < englishTriFreqDistHeur.N()*0.7)\n",
        "print('Number of Trigrams required for 70% coverage: ' + str(TriThresholdCountHeur))\n",
        "print('Unique Count: ', len(englishTriFreqDistHeur))\n",
        "print('5 Most Frequent occurring: ', englishTriFreqDistHeur.most_common(5))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "20065018\n",
            "20065016\n",
            "Number of Trigrams required for 70% coverage: 3865865\n",
            "Unique Count:  9885370\n",
            "5 Most Frequent occurring:  [(', and the', 15622), ('. in the', 14406), ('the age of', 12013), ('. there were', 10238), ('age of 18', 10080)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yu9SWzh00HmY"
      },
      "source": [
        "##### DATA ANALYSIS WITH STEMMING"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UiauD5BhYQ8G"
      },
      "source": [
        "englishTokensHeur = [ps.stem(word) for word in englishTokensHeur]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EYWcCcr208EJ"
      },
      "source": [
        "###### Unigrams required for 90% coverage"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SKYVnnDOYoxn",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "e07767d3-9389-4e31-8cb0-f5a6324ef88d"
      },
      "source": [
        "print(len(englishTokensHeur))\n",
        "englishUniGramsHeur = extractNGrams(englishTokensHeur, 1)\n",
        "print(len(englishUniGramsHeur))\n",
        "englishUniFreqDistHeur = nltk.FreqDist(englishUniGramsHeur)\n",
        "englishUniFreqListHeur = np.array(list(reversed(sorted([val for _, val in englishUniFreqDistHeur.items()]))))\n",
        "uniThresholdCountHeur = np.argmin(englishUniFreqListHeur.cumsum() < englishUniFreqDistHeur.N()*0.9)\n",
        "print('Number of Unigrams required for 90% coverage: ' + str(uniThresholdCountHeur))\n",
        "print('Unique Count: ', len(englishUniFreqDistHeur))\n",
        "print('5 Most Frequent occurring: ', englishUniFreqDistHeur.most_common(5))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "20065018\n",
            "20065018\n",
            "Number of Unigrams required for 90% coverage: 4705\n",
            "Unique Count:  246501\n",
            "5 Most Frequent occurring:  [('the', 1269620), (',', 1080241), ('.', 940699), ('of', 647353), ('and', 500680)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8h7-D2hA09Do"
      },
      "source": [
        "###### Bigrams required for 80% coverage"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jj7Bn1L8Ysgl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "5121aeef-a648-4b31-fda0-798487bb41d6"
      },
      "source": [
        "print(len(englishTokensHeur))\n",
        "englishBiGramsHeur = extractNGrams(englishTokensHeur, 2)\n",
        "print(len(englishBiGramsHeur))\n",
        "englishBiFreqDistHeur = nltk.FreqDist(englishBiGramsHeur)\n",
        "englishBiFreqListHeur = np.array(list(reversed(sorted([val for _, val in englishBiFreqDistHeur.items()]))))\n",
        "BiThresholdCountHeur = np.argmin(englishBiFreqListHeur.cumsum() < englishBiFreqDistHeur.N()*0.8)\n",
        "print('Number of Bigrams required for 80% coverage: ' + str(BiThresholdCountHeur))\n",
        "print('Unque Count: ', len(englishBiFreqDistHeur))\n",
        "print('5 Most Frequent occurring: ', englishBiFreqDistHeur.most_common(5))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "20065018\n",
            "20065017\n",
            "Number of Bigrams required for 80% coverage: 349518\n",
            "Unque Count:  3151801\n",
            "5 Most Frequent occurring:  [('of the', 180429), ('. the', 155682), (', and', 123743), ('in the', 120260), (\"' s\", 106041)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I5nOY1XO0-aY"
      },
      "source": [
        "###### Trigrams required for 70% coverage"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "up_Ar5LTYyfo",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "d735fbec-03fd-4617-d239-b8db3cb80ba8"
      },
      "source": [
        "print(len(englishTokensHeur))\n",
        "englishTriGramsHeur = extractNGrams(englishTokensHeur, 3)\n",
        "print(len(englishTriGramsHeur))\n",
        "englishTriFreqDistHeur = nltk.FreqDist(englishTriGramsHeur)\n",
        "englishTriFreqListHeur = np.array(list(reversed(sorted([val for _, val in englishTriFreqDistHeur.items()]))))\n",
        "TriThresholdCountHeur = np.argmin(englishTriFreqListHeur.cumsum() < englishTriFreqDistHeur.N()*0.7)\n",
        "print('Number of Trigrams required for 70% coverage: ' + str(TriThresholdCountHeur))\n",
        "print('Unique Count: ', len(englishTriFreqDistHeur))\n",
        "print('5 Most Frequent occurring: ', englishTriFreqDistHeur.most_common(5))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "20065018\n",
            "20065016\n",
            "Number of Trigrams required for 70% coverage: 3865865\n",
            "Unique Count:  9885370\n",
            "5 Most Frequent occurring:  [(', and the', 15622), ('. in the', 14406), ('the age of', 12013), ('. there were', 10238), ('age of 18', 10080)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mFJdiZ9cnSKA"
      },
      "source": [
        "#### 2 - Likelihood Ratio Test Implementation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_G8BX64_nf_v"
      },
      "source": [
        "##### Utility function for implementing algorithm"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xg6L2GJDnx_2"
      },
      "source": [
        "from math import log10\n",
        "\n",
        "def getVal(k, n, x):\n",
        "  temp = log10(x) * k\n",
        "  temp2 = log10(1-x) * (n-k)\n",
        "  temp += temp2\n",
        "  return temp\n",
        "\n",
        "def constructCollocations(bigram_dist, unigram_dist, number_tokens):\n",
        "    collocation = []\n",
        "    i = 0\n",
        "    for bigram, freq in bigram_dist.items():\n",
        "      bigramSplit = bigram.split()\n",
        "      c12 = freq\n",
        "      if len(bigramSplit) < 2:\n",
        "        continue\n",
        "\n",
        "      c1 = unigram_dist[bigramSplit[0]]\n",
        "      c2 = unigram_dist[bigramSplit[1]]\n",
        "      n = number_tokens\n",
        "      if c1 == 0 or c1 == n:\n",
        "        continue\n",
        "      \n",
        "      p = c2/n\n",
        "      p1 = c12/c1\n",
        "      p2 = (c2 - c12)/(n-c1)\n",
        "      if(p2 <= 0 or p1 <= 0 or p <= 0):\n",
        "          continue\n",
        "\n",
        "      if(p2 >= 1 or p1 >= 1 or p >= 1):\n",
        "          continue\n",
        "\n",
        "      val = getVal(c12, c1, p) + getVal(c2 - c12, n-c1, p) - getVal(c12, c1, p1) - getVal(c2 - c12, n-c1, p2)\n",
        "      val *= -2\n",
        "\n",
        "      if(val >= 7.88):\n",
        "          collocation.append(bigram)\n",
        "\n",
        "    return collocation"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YZ4r6OOCnntD"
      },
      "source": [
        "##### Analysis for English"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c0ZMbF11n2CZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "405f9d3c-f5e0-4563-d098-32a16ab79bb5"
      },
      "source": [
        "print('Type of bigrams as input:', len(englishBiFreqDist))\n",
        "print('Type of unigrams as input:', len(englishUniFreqDist))\n",
        "print('Number of unigrams as input:', len(englishUniGrams))\n",
        "collocations = constructCollocations(englishBiFreqDist, englishUniFreqDist, len(englishUniGrams))\n",
        "print('Number of collocations obtained:', len(collocations))\n",
        "print('Sample Ouput:', collocations[0:20])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Type of bigrams as input: 3977129\n",
            "Type of unigrams as input: 286686\n",
            "Number of unigrams as input: 20372526\n",
            "Number of collocations obtained: 491686\n",
            "Sample Ouput: ['The word', 'word \"', '\" was', 'was coined', 'coined by', 'ancient Greek', 'Greek philosophers', '. However', 'However ,', ', these', 'these ideas', 'ideas were', 'were founded', 'founded in', 'philosophical and', 'and theological', 'rather than', 'and experimentation', '. As', 'As a']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZpNb9n2MnqSW"
      },
      "source": [
        "##### Analysis for Hindi"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7cdeMAj2n2tv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "1fb1910e-4498-464c-b9e3-ee72a2a94439"
      },
      "source": [
        "print('Type of bigrams as input:', len(hindiBiFreqDist))\n",
        "print('Type of unigrams as input:', len(hindiUniFreqDist))\n",
        "print('Number of unigrams as input:', len(hindiUnigrams))\n",
        "collocations = constructCollocations(hindiBiFreqDist, hindiUniFreqDist, len(hindiUnigrams))\n",
        "print('Number of collocations obtained:', len(collocations))\n",
        "print('Sample Ouput:', collocations[0:20])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Type of bigrams as input: 2392979\n",
            "Type of unigrams as input: 322350\n",
            "Number of unigrams as input: 8640033\n",
            "Number of collocations obtained: 262625\n",
            "Sample Ouput: ['मास्टर ऑफ़', 'ऑफ़ हेल्थ', 'हेल्थ एडमिनिस्ट्रेशन', 'मास्टर ऑफ', 'हेल्थकेयर एडमिनिस्ट्रेशन', 'एडमिनिस्ट्रेशन (', 'एम .', '. एच', 'एच .', '. ए', 'स्नातकोत्तर (', 'पोस्ट ग्रेजुएशन', ') की', 'की एक', 'एक पेशेवर', 'है जो', 'स्वास्थ्य प्रशासन', 'प्रशासन के', 'के क्षेत्र', 'क्षेत्र में']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vLeY7bTkJTef"
      },
      "source": [
        "### 1.3.4 - Morphological parsing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6e2ZLQVfJg1m"
      },
      "source": [
        "#### 0 - PREREQUISITES - Installing required libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HX1_eWZz0LIs",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "dedb135b-ff3a-4bd3-cce8-27a188e76b3e"
      },
      "source": [
        "import random\n",
        "\n",
        "!sudo apt-get install python-numpy libicu-dev\n",
        "!pip install PyICU polyglot pycld2 Morfessor\n",
        "from polyglot.downloader import downloader\n",
        "!polyglot download morph2.en\n",
        "from polyglot.text import Word\n",
        "\n",
        "# !git clone https://github.com/anoopkunchukuttan/indic_nlp_resources.git\n",
        "from indicnlp.morph import unsupervised_morph \n",
        "from indicnlp import common\n",
        "common.INDIC_RESOURCES_PATH=\"/content/indic_nlp_resources\"\n",
        "analyzer = unsupervised_morph.UnsupervisedMorphAnalyzer('hi')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "python-numpy is already the newest version (1:1.13.3-2ubuntu1).\n",
            "libicu-dev is already the newest version (60.2-3ubuntu3.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 11 not upgraded.\n",
            "Requirement already satisfied: PyICU in /usr/local/lib/python3.6/dist-packages (2.5)\n",
            "Requirement already satisfied: polyglot in /usr/local/lib/python3.6/dist-packages (16.7.4)\n",
            "Requirement already satisfied: pycld2 in /usr/local/lib/python3.6/dist-packages (0.41)\n",
            "Requirement already satisfied: Morfessor in /usr/local/lib/python3.6/dist-packages (2.0.6)\n",
            "[polyglot_data] Downloading package morph2.en to\n",
            "[polyglot_data]     /root/polyglot_data...\n",
            "[polyglot_data]   Package morph2.en is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vG0bnHGqFFHy"
      },
      "source": [
        "#### Utility function (Sampling 5 most/least frequent words)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ew8TekRHM5TZ"
      },
      "source": [
        "def randomFreqUniGrams(freq_dist, n, m, isLeast):\n",
        "  randomUniGrams = []\n",
        "  if isLeast:\n",
        "    freqUniGrams = freq_dist.most_common()[-n:]\n",
        "    randomUniGrams = random.choices(freqUniGrams, k=m)\n",
        "  else:\n",
        "    freqUniGrams = freq_dist.most_common(n)\n",
        "    randomUniGrams = random.choices(freqUniGrams, k=m)\n",
        "  return randomUniGrams"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rAhT0btRPRAF"
      },
      "source": [
        "#### Morphological Analysis - English"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7MbWzyP3FXnk"
      },
      "source": [
        "##### Most Frequent words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FdfmRmWNznbb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "1fc02f3c-385b-4954-fa48-d22218c6b9cd"
      },
      "source": [
        "randomUniGrams = randomFreqUniGrams(englishUniFreqDist, 100, 5, False)\n",
        "randomFreqWords = []\n",
        "for word, freq in randomUniGrams:\n",
        "  randomFreqWords.append(word)\n",
        "print(randomFreqWords)\n",
        "for word in randomFreqWords:\n",
        "  word = Word(word, language=\"en\")\n",
        "  print(\"{:<20}{}\".format(word, word.morphemes))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['is', 'their', 'its', 'its', '-']\n",
            "is                  ['is']\n",
            "their               ['t', 'heir']\n",
            "its                 ['it', 's']\n",
            "its                 ['it', 's']\n",
            "-                   ['-']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vjW6vtUoFenL"
      },
      "source": [
        "##### Least Frequent words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qlFhpEK-0xe7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "d91e2b7a-10cf-4d54-cfc9-9226e3a87010"
      },
      "source": [
        "randomUniGrams = randomFreqUniGrams(englishUniFreqDist, 100, 5, True)\n",
        "randomWords = []\n",
        "for word, freq in randomUniGrams:\n",
        "  randomWords.append(word)\n",
        "print(randomWords)\n",
        "for word in randomWords:\n",
        "  word = Word(word, language=\"en\")\n",
        "  print(\"{:<20}{}\".format(word, word.morphemes))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['STAT', 'COPY', 'Sportstar', 'lication', 'featural']\n",
            "STAT                ['S', 'TA', 'T']\n",
            "COPY                ['CO', 'P', 'Y']\n",
            "Sportstar           ['S', 'port', 'star']\n",
            "lication            ['lic', 'ation']\n",
            "featural            ['feat', 'ural']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "62UxEgduvgTk"
      },
      "source": [
        "#### Morphological Analysis - Hindi\n",
        "Dependencies: hindiUniFreqDist"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yt9o1lsQFjTT"
      },
      "source": [
        "##### Most Frequent words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UtBw78101rT_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "cc850ad7-671f-4c15-85e6-0779dd73d374"
      },
      "source": [
        "randomUniGrams = randomFreqUniGrams(hindiUniFreqDist, 100, 5, False)\n",
        "randomFreqWords = []\n",
        "for word, freq in randomUniGrams:\n",
        "  randomFreqWords.append(word)\n",
        "print(randomFreqWords)\n",
        "for word in randomFreqWords:\n",
        "  hindiMorph = analyzer.morph_analyze(word)\n",
        "  print(\"{:<20}{}\".format(word, hindiMorph))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['उन्हें', 'उसके', 'दो', 'की', 'तो']\n",
            "उन्हें              ['उन्हें']\n",
            "उसके                ['उसके']\n",
            "दो                  ['दो']\n",
            "की                  ['की']\n",
            "तो                  ['तो']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M_F3MyoxF35T"
      },
      "source": [
        "##### Least Frequent words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0etir2InLu-L",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "8670eaf9-274a-40ef-ae47-314ae84a8dc3"
      },
      "source": [
        "randomUniGrams = randomFreqUniGrams(hindiUniFreqDist, 100, 5, True)\n",
        "randomWords = []\n",
        "for word, freq in randomUniGrams:\n",
        "  randomWords.append(word)\n",
        "print(randomWords)\n",
        "for word in randomWords:\n",
        "  hindiMorph = analyzer.morph_analyze(word)\n",
        "  print(\"{:<20}{}\".format(word, hindiMorph))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['ऑटोमिक्टिक', 'ऐरिनॉटोकस', 'लिंगनिश्चयन', 'मालकोविच', '\\n\\nप्लेबाय']\n",
            "ऑटोमिक्टिक          ['ऑटो', 'मिक', '्टिक']\n",
            "ऐरिनॉटोकस           ['ऐरि', 'नॉट', 'ो', 'कस']\n",
            "लिंगनिश्चयन         ['लिंग', 'निश्चय', 'न']\n",
            "मालकोविच            ['माल', 'कोविच']\n",
            "\n",
            "\n",
            "प्लेबाय           ['\\n\\nप्लेबाय']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XAH8pqNOObEX"
      },
      "source": [
        "### 1.3.5 - Sub-word Tokenization (Byte Pair Encoding)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fWLM3IJKTFoV"
      },
      "source": [
        "#### Installing required libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c1xbHwqKBaLI"
      },
      "source": [
        "import re\n",
        "from collections import Counter, defaultdict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_c04c8DDTNdX"
      },
      "source": [
        "#### BPE Algorithm Implementation for training data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aERrvW7_FLBg"
      },
      "source": [
        "#builds the vocab with the given freq distribution!!\n",
        "def buildVocabulary(freqDist):\n",
        "    vocab = nltk.FreqDist([])\n",
        "    for word, freq in freqDist.items():\n",
        "        w = ''\n",
        "        for c in word:\n",
        "            w += c + ' ' #Adding space between each character\n",
        "        w += '</w>' #Adding end of word\n",
        "        vocab[w] = freq\n",
        "    return vocab\n",
        "\n",
        "#returns the frequency of each pair for the vocabulary passed!\n",
        "def getVocabStats(vocab):\n",
        "    allPairs = defaultdict(int)\n",
        "    for word, freq in vocab.items():\n",
        "        symbols = word.split()\n",
        "        for i in range(len(symbols) - 1):\n",
        "            allPairs[symbols[i], symbols[i+1]] += freq\n",
        "    return allPairs\n",
        "\n",
        "#Accepts the best pair and vocabulary, and this function updates the vocab and outputs the updatedVocab\n",
        "def mergeVocabulary(pair, vocabIn):\n",
        "    vocabOut = nltk.FreqDist([])\n",
        "    bigram = re.escape(' '.join(pair))\n",
        "    p = re.compile(r'(?<!\\S)' + bigram + r'(?!\\S)')\n",
        "    for wordIn in vocabIn:\n",
        "        wordOut = p.sub(''.join(pair), wordIn)\n",
        "        vocabOut[wordOut] = vocabIn[wordIn]\n",
        "        \n",
        "    return vocabOut\n",
        "\n",
        "#Accepts the freqDist of the corpora and returns the training encoding(list of best pairs) with default iterations as 10\n",
        "def getTrainingEncodings(freqDist, numIterations=10):\n",
        "    vocab = buildVocabulary(freqDist)\n",
        "    trainEncodings = []\n",
        "\n",
        "    for i in range(numIterations):\n",
        "        pairs = getVocabStats(vocab)\n",
        "        if not pairs:\n",
        "            break\n",
        "        bestPair = max(pairs, key=pairs.get)\n",
        "        trainEncodings.append(bestPair)\n",
        "        vocab = mergeVocabulary(bestPair, vocab)\n",
        "    \n",
        "    return trainEncodings, vocab"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qx9A63zaTaOC"
      },
      "source": [
        "#### Utility functions for testing on 10 unknown words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0auCpaxlLC4n"
      },
      "source": [
        "#Testing functions\n",
        "def mergeVocabularyTest(pair, vocabIn):\n",
        "    vocabOut = []\n",
        "    bigram = re.escape(' '.join(pair))\n",
        "    p = re.compile(r'(?<!\\S)' + bigram + r'(?!\\S)')\n",
        "\n",
        "    for wordIn in vocabIn:\n",
        "        wordOut = p.sub(''.join(pair), wordIn)\n",
        "        vocabOut.append(wordOut)\n",
        "        \n",
        "    return vocabOut\n",
        "\n",
        "def tokenizeCorpus(corpus, trainEncodings):\n",
        "    vocabIn = [\" \".join(word) + \" </w>\" for word in corpus.split()]\n",
        "    for pair in trainEncodings:\n",
        "        vocabIn = mergeVocabularyTest(pair, vocabIn)\n",
        "\n",
        "    return vocabIn\n",
        "\n",
        "def helpMorpho(corpus, lang=\"English\"):\n",
        "    corpus = corpus.split()\n",
        "    if lang == \"English\":\n",
        "        for word in corpus:\n",
        "            word = Word(word, language=\"en\")\n",
        "            print(\"{:<20}{}\".format(word, word.morphemes))\n",
        "    if lang == \"Hindi\":\n",
        "        for word in corpus:\n",
        "            hin_morph = analyzer.morph_analyze(word)\n",
        "            print(\"{:<20}{}\".format(word, hin_morph))\n",
        "\n",
        "def helpBPE(corpus, pairEncoding):\n",
        "    testEncodings = tokenizeCorpus(corpus, pairEncoding)\n",
        "    corpus = corpus.split()\n",
        "    length = len(corpus)\n",
        "    for i in range(length):\n",
        "        word = corpus[i]\n",
        "        BPE_encoding = testEncodings[i]\n",
        "        BPE_encoding = BPE_encoding.split()\n",
        "        print(\"{:<20}{}\".format(word, BPE_encoding))\n",
        "\n",
        "def comparisons(corpus, encoding, lang=\"English\"):\n",
        "    print(\"Using Byte Pairing Encoding\")\n",
        "    helpBPE(corpus, encoding)\n",
        "\n",
        "    print(\"\\n\\nUsing inbuilt Morphological Analysis\")\n",
        "    helpMorpho(corpus, lang)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cRv3p5rhTkqL"
      },
      "source": [
        "#### COMPARISONS [BPE VS MORPHOLOGICAL] - ENGLISH"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wnis4oERLJ48",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 513
        },
        "outputId": "343ad73b-adc5-4249-cef4-f04593b9709f"
      },
      "source": [
        "print(\"ENGLISH CORPORA TRAINING\")\n",
        "englishTrainEncoding, trainingVocab = getTrainingEncodings(englishUniFreqDist,500)\n",
        "print(\"TRAINED ENCODINGS\")\n",
        "print(englishTrainEncoding[0:10])\n",
        "print(\"TOP 50 MOST FREQUENT WORDS\")\n",
        "print(trainingVocab.most_common(50))\n",
        "print(\"TOP 50 LEAST FREQUENT WORDS\")\n",
        "print(trainingVocab.most_common()[-50:])\n",
        "\n",
        "print(\"ENGLISH CORPORA TESTING\")\n",
        "englishCorpus = \"dehydrofreezing baronetised negroising nonconsequence autarkically serendipity gobbledygook scrumptious agastopia\"\n",
        "comparisons(englishCorpus, englishTrainEncoding, \"English\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\t\t\tENGLISH CORPORA TRAINING\n",
            "[('e', '</w>'), ('s', '</w>'), ('t', 'h'), ('d', '</w>'), ('n', '</w>'), ('e', 'r'), ('t', '</w>'), ('th', 'e</w>'), (',', '</w>'), ('i', 'n')]\n",
            "[('the</w>', 1084635), (',</w>', 994858), ('.</w>', 862133), ('of</w>', 646923), ('and</w>', 499999), ('in</w>', 379124), ('to</w>', 356109), ('a</w>', 334452), ('\"</w>', 308618), ('was</w>', 210438), ('The</w>', 185456), ('-</w>', 167996), ('is</w>', 164277), (\"'</w>\", 137466), ('as</w>', 135740), ('for</w>', 135273), ('(</w>', 124767), ('with</w>', 122627), ('by</w>', 112181), ('s</w>', 111365), ('that</w>', 111239), ('were</w>', 102633), ('on</w>', 101450), ('%</w>', 97183), ('from</w>', 94091), ('or</w>', 75555), ('his</w>', 72419), ('at</w>', 70801), ('In</w>', 66810), ('are</w>', 65130), ('an</w>', 64192), ('which</w>', 63647), ('had</w>', 58794), ('it</w>', 53681), ('be</w>', 53605), (')</w>', 52856), ('he</w>', 48977), ('has</w>', 40496), ('also</w>', 39104), ('\",</w>', 38576), ('not</w>', 38251), ('have</w>', 35905), ('who</w>', 35719), ('age</w>', 35458), ('their</w>', 34618), ('1</w>', 33976), (':</w>', 33077), ('but</w>', 32776), ('first</w>', 31967), ('2</w>', 31521)]\n",
            "[('V and en br in k</w>', 1), ('1 0 4 5 0</w>', 1), ('1 0 4 7 F </w>', 1), ('M ul ti l ing u al</w>', 1), ('C on S c ri p t</w>', 1), ('el li p tic iti es</w>', 1), ('E 7</w>', 1), ('M 4 9</w>', 1), ('M 5 9</w>', 1), ('M 8 7</w>', 1), ('4 1 2 5</w>', 1), ('S b c </w>', 1), ('S B b </w>', 1), ('M 3 1</w>', 1), ('M 1 0 4</w>', 1), ('M 5 1 a</w>', 1), ('M 9 1</w>', 1), ('M 9 5</w>', 1), ('N G C 1 6 7 2</w>', 1), ('2 5 3 6</w>', 1), ('2 9 0 3</w>', 1), ('S and age</w>', 1), ('M 8 6</w>', 1), ('5 8 6 6</w>', 1), ('S m</w>', 1), ('1 4 2 7 A</w>', 1), ('S T A T </w>', 1), ('u ti l il ty</w>', 1), ('bu il ti n</w>', 1), ('1 K B </w>', 1), ('2 K B </w>', 1), ('3 2 K B </w>', 1), ('l ic ation</w>', 1), ('D e bu g g ing</w>', 1), ('S mar t K ey</w>', 1), ('C O P Y </w>', 1), ('in iti iz ing</w>', 1), ('te ch i e</w>', 1), ('S h u g ar t</w>', 1), ('D E C sy st em</w>', 1), ('K il d all s</w>', 1), ('S of t C ard</w>', 1), ('l u g g ab l es</w>', 1), ('P C W </w>', 1), ('M ul ti pl an</w>', 1), ('T el en g ard</w>', 1), ('G or ill as</w>', 1), ('H am ur ab i</w>', 1), ('X M O D E M </w>', 1), ('5 ¼ </w>', 1)]\n",
            "\n",
            "\t\t\tENGLISH CORPORA TESTING\n",
            "Using Byte Pairing Encoding\n",
            "dehydrofreezing     ['de', 'h', 'y', 'd', 'ro', 'f', 're', 'e', 'z', 'ing</w>']\n",
            "baronetised         ['b', 'ar', 'on', 'e', 'ti', 's', 'ed</w>']\n",
            "negroising          ['n', 'eg', 'ro', 'is', 'ing</w>']\n",
            "nonconsequence      ['n', 'on', 'con', 'se', 'qu', 'ence</w>']\n",
            "autarkically        ['a', 'ut', 'ar', 'k', 'ic', 'ally</w>']\n",
            "serendipity         ['ser', 'en', 'di', 'p', 'ity</w>']\n",
            "gobbledygook        ['g', 'ob', 'b', 'le', 'd', 'y', 'g', 'oo', 'k</w>']\n",
            "scrumptious         ['sc', 'r', 'um', 'p', 'ti', 'ous</w>']\n",
            "agastopia           ['ag', 'ast', 'op', 'ia</w>']\n",
            "\n",
            "\n",
            "Using inbuilt Morphological Analysis\n",
            "dehydrofreezing     ['de', 'hydro', 'free', 'z', 'ing']\n",
            "baronetised         ['baron', 'et', 'ised']\n",
            "negroising          ['negro', 'ising']\n",
            "nonconsequence      ['non', 'con', 'sequence']\n",
            "autarkically        ['aut', 'ark', 'ical', 'ly']\n",
            "serendipity         ['s', 'er', 'en', 'dip', 'ity']\n",
            "gobbledygook        ['go', 'b', 'ble', 'dy', 'go', 'o', 'k']\n",
            "scrumptious         ['s', 'c', 'rump', 't', 'ious']\n",
            "agastopia           ['a', 'ga', 'stop', 'ia']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lq3iQjGRTwzy"
      },
      "source": [
        "#### COMPARISONS [BPE VS MORPHOLOGICAL] - HINDI"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c8dxo-maLK0I",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 547
        },
        "outputId": "8823d851-de40-409d-8d1f-2e52f25c2104"
      },
      "source": [
        "print(\"tHINDI CORPORA TRAINING\")\n",
        "hindiTrainEncoding, hindiTrainVocab = getTrainingEncodings(hindiUniFreqDist, 500)\n",
        "print(\"TRAINED ENCODINGS\")\n",
        "print(hindiTrainEncoding[0:10])\n",
        "print(\"TOP 50 MOST FREQUENT WORDS\")\n",
        "print(hindiTrainVocab.most_common(50))\n",
        "print(\"TOP 50 LEAST FREQUENT WORDS\")\n",
        "print(hindiTrainVocab.most_common()[-50:])\n",
        "\n",
        "\n",
        "print(\"\\n\\t\\t\\tHINDI CORPORA TESTING\")\n",
        "hindiCorpus = \" संपादन दस्ता असहिष्णुता प्रत्येक बंदरगाह कुलाधिपति अधिनियम आवेग अभियांत्रिकी वैतरणी\"\n",
        "comparisons(hindiCorpus, hindiTrainEncoding, \"Hindi\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\t\t\tHINDI CORPORA TRAINING\n",
            "[('े', '</w>'), ('ा', '</w>'), ('ी', '</w>'), ('ं', '</w>'), ('र', '</w>'), ('क', 'े</w>'), ('्', 'र'), ('न', '</w>'), ('ह', 'ै'), ('।', '</w>')]\n",
            "[('के</w>', 357833), ('।</w>', 321526), ('में</w>', 268249), (',</w>', 267743), ('है</w>', 232156), ('की</w>', 171779), ('और</w>', 155743), ('से</w>', 134901), ('का</w>', 122948), ('को</w>', 115876), ('हैं</w>', 85787), ('\"</w>', 80300), ('एक</w>', 80221), ('-</w>', 75215), (')</w>', 70811), ('(</w>', 70320), ('पर</w>', 64984), ('ने</w>', 54126), ('किया</w>', 53857), ('लिए</w>', 52226), ('भी</w>', 47609), ('.</w>', 47105), ('था</w>', 43005), ('कि</w>', 39996), ('गया</w>', 39123), ('यह</w>', 37774), ('इस</w>', 34164), ('रूप</w>', 32207), ('जाता</w>', 28738), ('ही</w>', 28698), ('जो</w>', 28627), ('करने</w>', 28440), ('साथ</w>', 28354), ('कर</w>', 28043), ('हो</w>', 27564), ('नहीं</w>', 26563), ('द्वारा</w>', 22877), ('या</w>', 22636), (\"'</w>\", 21649), ('थे</w>', 21405), ('तथा</w>', 21081), ('अपने</w>', 19993), ('बाद</w>', 19818), ('तक</w>', 19168), ('दिया</w>', 18771), (':</w>', 18034), ('होता</w>', 17543), ('थी</w>', 16868), ('वह</w>', 15920), ('कुछ</w>', 14228)]\n",
            "[('\\n \\n लि ंग नि श् च य</w>', 1), ('ऐ रि न ॉ ट ो की</w>', 1), ('स् त्र ी जन न</w>', 1), ('थ े लि ओ ट ो की</w>', 1), ('उ भ य जन न</w>', 1), ('डे ं ट रो ट ो की</w>', 1), ('ऐ ं फ़ ि ट ो की</w>', 1), ('\\n \\n को शि का त त्व</w>', 1), ('\\n \\n अ र् ध क</w>', 1), ('\\n \\n त न ू</w>', 1), ('के ं द्र क सू त्र ों</w>', 1), ('\\n \\n स्व त स् सं से च क</w>', 1), ('ऑ ट ो मि क् ट िक</w>', 1), ('सि न ै प् सि स</w>', 1), ('अ र् ध के ं द्र कों</w>', 1), ('न ्यू क् लि आ ई</w>', 1), ('फ़ ्यू ज् ह न</w>', 1), ('रे स्ट ि ट ्यू टे ड</w>', 1), ('अं त र् भा जन</w>', 1), ('ए ंड ो मा इ ट ो सि स</w>', 1), ('\\n \\n अ मै थ ु नी</w>', 1), ('ऐ पो मि क् ट िक</w>', 1), ('ऐ रि न ॉ ट ो क स</w>', 1), ('शे वे</w>', 1), ('စ ဝ ် </w>', 1), ('ရ ွ ှ ေ </w>', 1), ('သ ိ ု က ် </w>', 1), ('s a ʔ </w>', 1), ('ʃ w è </w>', 1), ('θ a i ʔ </w>', 1), ('पि छ ले S a o p h a </w>', 1), ('Y a w n g h w e</w>', 1), ('कं बा स रह ता</w>', 1), ('थ ी री</w>', 1), ('प व ार महा व न् था</w>', 1), ('थ ु द म र ज़ ा</w>', 1), ('न ्य ंग</w>', 1), ('यो व घ वे</w>', 1), ('ह ॉ क में</w>', 1), ('स्था न अ ब</w>', 1), ('\\n \\n ओ \\n पर</w>', 1), ('\\xa0 शे वे</w>', 1), ('T a u n g g y i </w>', 1), ('यो व घ ् वे</w>', 1), ('सो फ ा</w>', 1), ('ह क् कम</w>', 1), ('ह से न वी</w>', 1), ('स ौ फ ा</w>', 1), ('\\n \\n शे वे</w>', 1), ('स्व त ं</w>', 1)]\n",
            "\n",
            "\t\t\tHINDI CORPORA TESTING\n",
            "Using Byte Pairing Encoding\n",
            "संपादन              ['सं', 'पा', 'द', 'न</w>']\n",
            "दस्ता               ['द', 'स्', 'ता</w>']\n",
            "असहिष्णुता          ['अ', 'स', 'हि', 'ष्', 'ण', 'ु', 'ता</w>']\n",
            "प्रत्येक            ['प्र', 'त्य', 'ेक</w>']\n",
            "बंदरगाह             ['बं', 'द', 'र', 'गा', 'ह</w>']\n",
            "कुलाधिपति           ['कु', 'ला', 'ध', 'ि', 'प', 'ति</w>']\n",
            "अधिनियम             ['अधि', 'निय', 'म</w>']\n",
            "आवेग                ['आ', 'वे', 'ग</w>']\n",
            "अभियांत्रिकी        ['अ', 'भ', 'िया', 'ं', 'त्र', 'ि', 'की</w>']\n",
            "वैतरणी              ['वै', 'तर', 'ण', 'ी</w>']\n",
            "\n",
            "\n",
            "Using inbuilt Morphological Analysis\n",
            "संपादन              ['संपादन']\n",
            "दस्ता               ['दस्ता']\n",
            "असहिष्णुता          ['अ', 'सहिष्णु', 'ता']\n",
            "प्रत्येक            ['प्रत्येक']\n",
            "बंदरगाह             ['बंदरगाह']\n",
            "कुलाधिपति           ['कुला', 'धिपति']\n",
            "अधिनियम             ['अधिनियम']\n",
            "आवेग                ['आवेग']\n",
            "अभियांत्रिकी        ['अभि', 'यांत्रिक', 'ी']\n",
            "वैतरणी              ['वै', 'तरणी']\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}