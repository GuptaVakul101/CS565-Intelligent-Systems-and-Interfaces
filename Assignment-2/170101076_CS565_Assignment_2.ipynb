{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "170101076-CS565-Assignment-2.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XpBxe2iOYp6x"
      },
      "source": [
        "#### **VAKUL GUPTA - 170101076**\n",
        "# **INTELLIGENT SYSTEMS AND INTERFACES - CS565 - ASSIGNMENT 2**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qXbQd5IiaSs_"
      },
      "source": [
        "### 0 - PREREQUISITE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W_GMB3ql_5Nw"
      },
      "source": [
        "##### Mount google drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o2Hpc2raYtUf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d211e969-ac27-44f1-91cb-0a4ea72ba438"
      },
      "source": [
        "# mount google drive\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-VIoPLW7_-yh"
      },
      "source": [
        "##### Install basic libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KFL_jSQXaEN0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d132fbcb-3326-40f0-a3ea-7a1454aba542"
      },
      "source": [
        "# install basic libraries\n",
        "import random\n",
        "import nltk\n",
        "import codecs\n",
        "import math\n",
        "import statistics\n",
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mu6YRoIjAI--"
      },
      "source": [
        "##### Read and load english dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nSTJujAIZBvD"
      },
      "source": [
        "# read and load english corpus\n",
        "englishText = codecs.open('/content/drive/My Drive/Intelligent_Systems_CS565/Assignment-2/en_wiki.txt', 'r').read()\n",
        "\n",
        "# filter corpus to take a percantage of it\n",
        "filterRatio = 0.1\n",
        "englishText = englishText[0:int(filterRatio*len(englishText))]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V17rHt5FacAR"
      },
      "source": [
        "##### Sentence Segmentation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Jc9II8hag7d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e4abe70c-f1d5-4996-ab08-e80bd145e6b4"
      },
      "source": [
        "# sentence segmentation\n",
        "listOfEnglishSentences = []\n",
        "listOfSentences = nltk.sent_tokenize(englishText)\n",
        "for sentence in listOfSentences:\n",
        "    listOfEnglishSentences.append(sentence)\n",
        "print(len(listOfEnglishSentences))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "761582\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5HmfCCYva48r"
      },
      "source": [
        "##### Word Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3QU8QCD_bA7M"
      },
      "source": [
        "# function to convert a sentence into a list of words\n",
        "def performWordTokenisation(sentences):\n",
        "    wordTokens = []\n",
        "    for sentence in sentences:\n",
        "        words = nltk.word_tokenize(sentence)\n",
        "        for word in words:\n",
        "            wordTokens.append(word)\n",
        "    \n",
        "    return wordTokens"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gKsJsW1Db3qe"
      },
      "source": [
        "##### Split Data into training, devlopment and test sets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tW0xQi5kb9dC"
      },
      "source": [
        "# split data into part-1 (90%) and part-2 (10%)\n",
        "numberOfTokens = len(listOfEnglishSentences)\n",
        "# randomly shuffle sentences for splitting\n",
        "random.shuffle(listOfEnglishSentences)\n",
        "datasetTrainAndDevelopment = listOfEnglishSentences[:int((numberOfTokens+1)*.9)]\n",
        "part2 = listOfEnglishSentences[int((numberOfTokens+1)*.9):]\n",
        "# fixed test set\n",
        "datasetTest = performWordTokenisation(part2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vvxC7-dcCFq_"
      },
      "source": [
        "##### Utility Function for splitting part-1 data into training and development sets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vtuQtzxVEClE"
      },
      "source": [
        "# given part-1 split randomly into training set (90%) and development set (10%)\n",
        "def getTrainingAndDevelopmentSet(datasetTrainAndDevelopment):\n",
        "    random.shuffle(datasetTrainAndDevelopment)\n",
        "    datasetTrain = datasetTrainAndDevelopment[: int( (len(datasetTrainAndDevelopment)+1)*.9)]\n",
        "    datasetDevelopment = datasetTrainAndDevelopment[int( (len(datasetTrainAndDevelopment)+1)*.9):]\n",
        "    datasetTrain=performWordTokenisation(datasetTrain)\n",
        "    datasetDevelopment=performWordTokenisation(datasetDevelopment)\n",
        "    return datasetTrain, datasetDevelopment"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zokb02ZXKgHH"
      },
      "source": [
        "### 1 - N-GRAM LANGUAGE MODEL"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U7ukLhbiSv_X"
      },
      "source": [
        "#### 1.1 Implement Discounting and Interpolation Smoothing Methods"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pej95eXWEt5n"
      },
      "source": [
        "###### Utility functions for obtaining Ngram unsmoothed probabilities"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ii5_i9NXHo_m"
      },
      "source": [
        "# get unsmoothed probability for unigrams using prob = frequency of a particular unigram / total\n",
        "def getProbUnigramDict(freqOfUnigramDict):\n",
        "    totalNumberOfTokens = 0\n",
        "    for item in freqOfUnigramDict:\n",
        "        totalNumberOfTokens += freqOfUnigramDict[item]\n",
        "    probUnigramDict = {}\n",
        "    for unigram in freqOfUnigramDict:\n",
        "        probUnigramDict[unigram] = (freqOfUnigramDict.get(unigram)) / totalNumberOfTokens\n",
        "    return probUnigramDict\n",
        "\n",
        "# get unsmoothed probability for bigrams using prob = frequency of a particular bigram / total\n",
        "def getProbBigramDict(allBigramsList, freqOfUnigramDict, freqOfBigramDict):\n",
        "    probBigramDict = {}\n",
        "    for bigram in allBigramsList:\n",
        "        probBigramDict[bigram] = (freqOfBigramDict.get(bigram)) / (freqOfUnigramDict.get(bigram[0]))\n",
        "    return probBigramDict\n",
        "\n",
        "# get unsmoothed probability for trigrams using prob = frequency of a particular trigram / total\n",
        "def getProbTrigramDict(allTrigramsList, freqOfBigramDict, freqOfTrigramDict):\n",
        "    probTrigramDict = {}\n",
        "    for trigram in allTrigramsList:\n",
        "        probTrigramDict[trigram] = (freqOfTrigramDict.get(trigram)) / (freqOfBigramDict.get((trigram[0], trigram[1])))\n",
        "    return probTrigramDict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uHg7mBKLJ_bc"
      },
      "source": [
        "###### Utility function for obtaining Ngram list and frequencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4wZj03-sFFFL"
      },
      "source": [
        "# get all the details of the dataset including -\n",
        "# list of all bigrams and trigrams\n",
        "# dictionaries containing frequency of unique unigrams, bigrams and trigrams\n",
        "def getFreqAndListNgrams(datasetTrain):\n",
        "    # frequency dictionaries of N-grams\n",
        "    freqOfBigramDict = {}\n",
        "    freqOfUnigramDict = {}\n",
        "    freqOfTrigramDict = {}\n",
        "\n",
        "    # list of all bigrams and trigrams\n",
        "    allBigramsList = []\n",
        "    allTrigramsList = []\n",
        "\n",
        "    for i in range(len(datasetTrain)):\n",
        "        if i < len(datasetTrain) - 1:\n",
        "            allBigramsList.append((datasetTrain[i], datasetTrain[i + 1]))\n",
        "            if (datasetTrain[i], datasetTrain[i + 1]) in freqOfBigramDict:\n",
        "                freqOfBigramDict[(datasetTrain[i], datasetTrain[i + 1])] += 1\n",
        "            else:\n",
        "                freqOfBigramDict[(datasetTrain[i], datasetTrain[i + 1])] = 1\n",
        "\n",
        "            if i < len(datasetTrain) - 2:\n",
        "                allTrigramsList.append((datasetTrain[i], datasetTrain[i + 1], datasetTrain[i + 2]))\n",
        "                if (datasetTrain[i], datasetTrain[i + 1], datasetTrain[i + 2]) in freqOfTrigramDict:\n",
        "                    freqOfTrigramDict[(datasetTrain[i], datasetTrain[i + 1], datasetTrain[i + 2])] += 1\n",
        "                else:\n",
        "                    freqOfTrigramDict[(datasetTrain[i], datasetTrain[i + 1], datasetTrain[i + 2])] = 1\n",
        "\n",
        "        if datasetTrain[i] in freqOfUnigramDict:\n",
        "            freqOfUnigramDict[datasetTrain[i]] += 1\n",
        "        else:\n",
        "            freqOfUnigramDict[datasetTrain[i]] = 1\n",
        "\n",
        "    return freqOfUnigramDict, freqOfBigramDict, freqOfTrigramDict, allBigramsList, allTrigramsList"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9MvWIAOlKNT7"
      },
      "source": [
        "###### Utility function for getting all Ngram Details"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4rISzA7bHxWQ"
      },
      "source": [
        "# function for obtaining probabilities, and frequency distributions of all unigrams, bigrams and trigrams of a particular dataset\n",
        "def getAlldatasetTrainDetails(datasetTrain):\n",
        "    freqOfUnigramDict, freqOfBigramDict, freqOfTrigramDict, allBigramsList, allTrigramsList = getFreqAndListNgrams(datasetTrain)\n",
        "    probUnigramDict = getProbUnigramDict(freqOfUnigramDict)\n",
        "    probBigramDict = getProbBigramDict(allBigramsList, freqOfUnigramDict, freqOfBigramDict)\n",
        "    probTrigramDict = getProbTrigramDict(allTrigramsList, freqOfBigramDict, freqOfTrigramDict)\n",
        "    return freqOfUnigramDict, freqOfBigramDict, freqOfTrigramDict, allBigramsList, allTrigramsList, probUnigramDict, probBigramDict, probTrigramDict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-wUmiYQ7k4yx"
      },
      "source": [
        "###### DISCOUNTING IMPLEMENTATION"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bTynnnMqOb-T"
      },
      "source": [
        "###### Utility function for obtaining trigram discounted smoothed probabilities"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bP1IZ1VVONos"
      },
      "source": [
        "# function for calculating probabilities of trigrams after disconting\n",
        "def getDiscountedProbTrigramDict(allTrigramsList, freqOfTrigramDict, freqOfBigramDict, beta):\n",
        "    discountedProbTrigramDict = {}\n",
        "    for trigram in allTrigramsList:\n",
        "        # the corresponding count is decreased by beta amount\n",
        "        count = (freqOfTrigramDict.get(trigram) - beta)\n",
        "        if count < 0:\n",
        "            count = 0\n",
        "        discountedProbTrigramDict[trigram] = count / freqOfBigramDict.get((trigram[0], trigram[1]))\n",
        "    \n",
        "    return discountedProbTrigramDict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BYX5mEwsMyyk"
      },
      "source": [
        "###### Utility function for obtaining perplexity after discounting method"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KccquSnwersE"
      },
      "source": [
        "# calculates perplexity (evaluation performance) on test data after discounting method\n",
        "def evaluatePerplexityDiscounting(datasetTest, freqOfTrigramDict, discountedProbTrigramDict):\n",
        "    total = 0\n",
        "    # summation of logarithm of discounted probabilities\n",
        "    sumOfLogProb = 0\n",
        "    totalTrigrams = 0\n",
        "    for item in freqOfTrigramDict:\n",
        "        totalTrigrams += freqOfTrigramDict[item]\n",
        "    for j in range(len(datasetTest) - 2):\n",
        "        trigram = (datasetTest[j], datasetTest[j + 1], datasetTest[j + 2])\n",
        "        if trigram in discountedProbTrigramDict:\n",
        "            prob = discountedProbTrigramDict[trigram]\n",
        "        else:\n",
        "            prob = freqOfTrigramDict[trigram] / totalTrigrams\n",
        "\n",
        "        if prob == 0:\n",
        "            sumOfLogProb += 0\n",
        "        else:\n",
        "            sumOfLogProb += math.log(prob, 2)\n",
        "        total += 1\n",
        "    # calculates the entropy\n",
        "    entropy = (-1 * sumOfLogProb ) / total\n",
        "    # perplexity is 2^entropy\n",
        "    perplexity = math.pow(2, entropy)\n",
        "    return perplexity"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t9k5TCRNM_FO"
      },
      "source": [
        "###### Utility function for finding optimal parameter and corresponding perplexity"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WFapZq54M9zP"
      },
      "source": [
        "# finds the log likelihood value for a particular beta\n",
        "def getLogLikelihoodVal(beta, datasetTrain, datasetTest):\n",
        "    freqOfUnigramDict, freqOfBigramDict, freqOfTrigramDict, allBigramsList, allTrigramsList, probUnigramDict, probBigramDict, probTrigramDict = getAllTrainDataDetails(datasetTrain)\n",
        "    freqOfUnigramDictTest, freqOfBigramDictTest, freqOfTrigramDictTest, allBigramsListTest, allTrigramsListTest = getFreqAndListNgrams(datasetTest)\n",
        "\n",
        "    curLikelihood = 0\n",
        "    tokens = 0\n",
        "    for trigram, freq in freqOfTrigramDict.items():\n",
        "        curProb = probTrigramDict[trigram]\n",
        "        tokens += freq\n",
        "\n",
        "        if curProb != 0:\n",
        "            curLikelihood += (freq * math.log(curProb, 2))\n",
        "    \n",
        "    return curLikelihood\n",
        "\n",
        "# find the optimal parameter by maximizing the log likelihood test \n",
        "def findOptimalBeta(datasetTrain, datasetTest):\n",
        "    beta = 0.1\n",
        "    optimalBeta = 0\n",
        "    bestVal = 0\n",
        "    while beta <= 1:\n",
        "        tempVal = getLogLikelihoodVal(beta, datasetTrain, datasetTest)\n",
        "\n",
        "        if tempVal > bestVal:\n",
        "            bestVal = tempVal\n",
        "            optimalBeta = beta\n",
        "        beta += 0.1\n",
        "\n",
        "    return optimalBeta\n",
        "\n",
        "# find optimal beta and calculate perplexity after discounting for that optimal parameter\n",
        "def findOptimalBetaAndPerplexityDiscounting(datasetTrain, datasetTest):\n",
        "    freqOfUnigramDict, freqOfBigramDict, freqOfTrigramDict, allBigramsList, allTrigramsList, probUnigramDict, probBigramDict, probTrigramDict = getAlldatasetTrainDetails(datasetTrain)\n",
        "    freqOfUnigramDictTest, freqOfBigramDictTest, freqOfTrigramDictTest, allBigramsListTest, allTrigramsListTest = getFreqAndListNgrams(datasetTest)\n",
        "\n",
        "    optimalBeta = findOptimalBeta(datasetTrain,datasetTest)\n",
        "    discountedProbTrigramDict = getDiscountedProbTrigramDict(allTrigramsList, freqOfTrigramDict, freqOfBigramDict, optimalBeta)\n",
        "    perplexity = evaluatePerplexityDiscounting(datasetTest, freqOfTrigramDictTest, discountedProbTrigramDict)\n",
        "\n",
        "    return optimalBeta, perplexity"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zBKHfuhDkzJk"
      },
      "source": [
        "###### INTERPOLATION IMPLEMENTATION"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qNRIOa82vXK1"
      },
      "source": [
        "###### Utility function for finding optimal parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B-yIsqV0wIf4"
      },
      "source": [
        "# finds the optimal parameter set (lambda1,lambda2,lambda3)\n",
        "def findOptimalParameters(tokens, epsilon, probUnigramDict, probBigramDict, probTrigramDict):\n",
        "    parameterSet = [0.2, 0.3, 0.5]\n",
        "    # repeatedly perform a certain process\n",
        "    while 1:\n",
        "        countExpected = [0, 0, 0]\n",
        "        for j in range(len(tokens) - 2):\n",
        "            trigram = (tokens[j], tokens[j + 1], tokens[j + 2])\n",
        "            bigram = (tokens[j], tokens[j + 1])\n",
        "            tProb = probTrigramDict.get(trigram, 0)\n",
        "            bProb = probBigramDict.get(bigram, 0)\n",
        "            uProb = probUnigramDict.get(tokens[j], 0)\n",
        "            weightedProb = parameterSet[0] * uProb + parameterSet[1] * bProb + parameterSet[2] * tProb\n",
        "            if weightedProb > 0:\n",
        "                countExpected[0] += (parameterSet[0] * uProb) / weightedProb\n",
        "                countExpected[1] += (parameterSet[1] * bProb) / weightedProb\n",
        "                countExpected[2] += (parameterSet[2] * tProb) / weightedProb\n",
        "        \n",
        "        newParameterSet = [0,0,0]\n",
        "        sumTotal = countExpected[0] + countExpected[1] + countExpected[2] \n",
        "        if sumTotal == 0:\n",
        "            return parameterSet\n",
        "\n",
        "        # finds the new parameter set using expected counts\n",
        "        newParameterSet[0] = countExpected[0] / sumTotal\n",
        "        newParameterSet[1] = countExpected[1] / sumTotal\n",
        "        newParameterSet[2] = countExpected[2] / sumTotal\n",
        "\n",
        "        # check if all the three parameters differs less than epsilon from the current parameter set \n",
        "        isSaturated = True\n",
        "        for i in range(0,3):\n",
        "            diff = abs( newParameterSet[i] - parameterSet[i] )\n",
        "            if diff > epsilon:\n",
        "                isSaturated = False\n",
        "                break\n",
        "        if isSaturated == True:\n",
        "            return parameterSet\n",
        "        else:\n",
        "            # if not saturated, update the parameter set and repeat the process again\n",
        "            parameterSet = newParameterSet"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hiHhBInmPOWV"
      },
      "source": [
        "###### Utility function for obtaining perplexity after interpolation method"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "49hvbRXPPTGi"
      },
      "source": [
        "# calculates the perplexity after interpolation method\n",
        "def evaluatePerplexityInterpolation(tokens, parameterSet, probUnigramDict, probBigramDict, probTrigramDict):\n",
        "    # summation of logarithm of discounted probabilities\n",
        "    sumOfLogProb = 0\n",
        "    for j in range(len(tokens) - 2):\n",
        "        trigram = (tokens[j], tokens[j + 1], tokens[j + 2])\n",
        "        bigram = (tokens[j], tokens[j + 1])\n",
        "        tProb = probTrigramDict.get(trigram, 0)\n",
        "        bProb = probBigramDict.get(bigram, 0)\n",
        "        uProb = probUnigramDict.get(tokens[j], 0)\n",
        "        weightedProb = parameterSet[0] * uProb + parameterSet[1] * bProb + parameterSet[2] * tProb\n",
        "        if weightedProb == 0:\n",
        "            sumOfLogProb += 0\n",
        "        else:\n",
        "            sumOfLogProb += math.log(weightedProb, 2)\n",
        "    totalNumberOfTrigrams = len(tokens) - 2;\n",
        "    # calculates the entropy\n",
        "    entropy = (-1 * sumOfLogProb ) / totalNumberOfTrigrams\n",
        "    # perplexity is 2^entropy\n",
        "    perplexity = math.pow(2, entropy)\n",
        "    return perplexity"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uYrHtuQIPBjh"
      },
      "source": [
        "###### Utility function for finding optimal parameters and corresponding perplexity"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F8iB_5_bPHI9"
      },
      "source": [
        "# get optimal parameters and the perplexity value on those parameters\n",
        "def getLambdaAndPerplexityInterpolation(datasetTrain, datasetTest):\n",
        "    freqOfUnigramDict, freqOfBigramDict, freqOfTrigramDict, allBigramsList, allTrigramsList, probUnigramDict, probBigramDict, probTrigramDict = getAlldatasetTrainDetails(datasetTrain)\n",
        "    parameterSet = findOptimalParameters(datasetTest, 0.005, probUnigramDict, probBigramDict, probTrigramDict)\n",
        "    perplexity = evaluatePerplexityInterpolation(datasetTest, parameterSet, probUnigramDict, probBigramDict, probTrigramDict)\n",
        "    return parameterSet, perplexity"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1esEzPcfNQG-"
      },
      "source": [
        "#### 1.2 Split Part-1 Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M82InUD0NS6b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b8329cd4-3c4e-46fb-8fac-e8bfb08021ba"
      },
      "source": [
        "# split the part-1 data into five set of training and development data randomly shuffled\n",
        "datasetTrainList = []\n",
        "datasetDevelopmentList = []\n",
        "for iteration in range(0,5):\n",
        "    datasetTrain, datasetDevelopment = getTrainingAndDevelopmentSet(datasetTrainAndDevelopment)\n",
        "    print('Dataset Train',iteration+1)\n",
        "    print(datasetTrain[0:5])\n",
        "    print('Dataset Development',iteration+1)\n",
        "    print(datasetDevelopment[0:5])\n",
        "    datasetTrainList.append(datasetTrain)\n",
        "    datasetDevelopmentList.append(datasetDevelopment)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dataset Train 1\n",
            "['Males', 'had', 'a', 'median', 'income']\n",
            "Dataset Development 1\n",
            "['``', 'The', 'World', 'Tonight', \"''\"]\n",
            "Dataset Train 2\n",
            "['A', '``', 'Draft', 'Eisenhower', \"''\"]\n",
            "Dataset Development 2\n",
            "['With', 'the', 'exception', 'of', '2009']\n",
            "Dataset Train 3\n",
            "['Females', 'have', 'two', 'of', 'the']\n",
            "Dataset Development 3\n",
            "['Primary', 'and', 'secondary', 'education', 'is']\n",
            "Dataset Train 4\n",
            "['As', 'the', 'fears', 'of', 'a']\n",
            "Dataset Development 4\n",
            "['The', 'woman', ',', 'a', 'journalist']\n",
            "Dataset Train 5\n",
            "['In', 'the', 'Shiva', 'Purana', ',']\n",
            "Dataset Development 5\n",
            "['Not', 'only', 'the', 'first', 'use']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tCD28GXFNmC7"
      },
      "source": [
        "#### 1.3 Model's Performance on Development Set"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cOT-XAFUPOBA"
      },
      "source": [
        "###### Optimal Parameters and Perplexity - *Interpolation*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jzpikW-DNq3f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "54c7973a-baa2-4daf-938f-8e8d2bb7f4ff"
      },
      "source": [
        "# finds the optimal parameter set and the corresponding perplexity on five different development sets for interpolation\n",
        "for iteration in range(0,5):\n",
        "    parameterSet, perplexityInterpolation = getLambdaAndPerplexityInterpolation(datasetTrainList[iteration], datasetDevelopmentList[iteration])\n",
        "    print('Iteration: ',iteration+1)\n",
        "    print('Optimal paramater set: ',parameterSet)\n",
        "    print('Perplexity Interpolation: ', perplexityInterpolation)\n",
        "    print('\\n')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration:  1\n",
            "Optimal paramater set:  [0.32164648269469676, 0.4108400100831291, 0.26751350722217404]\n",
            "Perplexity Interpolation:  83.33429776721303\n",
            "\n",
            "\n",
            "Iteration:  2\n",
            "Optimal paramater set:  [0.3218061078363997, 0.41100600387609304, 0.2671878882875072]\n",
            "Perplexity Interpolation:  83.47264677065282\n",
            "\n",
            "\n",
            "Iteration:  3\n",
            "Optimal paramater set:  [0.3223139417159694, 0.4097407533483232, 0.26794530493570745]\n",
            "Perplexity Interpolation:  83.51132068676277\n",
            "\n",
            "\n",
            "Iteration:  4\n",
            "Optimal paramater set:  [0.3213984794067617, 0.41077001648718775, 0.26783150410605056]\n",
            "Perplexity Interpolation:  83.10668348051617\n",
            "\n",
            "\n",
            "Iteration:  5\n",
            "Optimal paramater set:  [0.3209134531994651, 0.41041508808930366, 0.26867145871123127]\n",
            "Perplexity Interpolation:  82.65508917158509\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EfLhZmttPV8P"
      },
      "source": [
        "###### Optimal Parameters and Perplexity - *Discounting*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HJzUs1hDP_mR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "12e20c28-2c5f-4cf6-8a8a-8834f9830b46"
      },
      "source": [
        "# finds the optimal beta and the corresponding perplexity on five different development sets for discounting\n",
        "for iteration in range(0,5):\n",
        "    beta, perplexityDiscounting = findOptimalBetaAndPerplexityDiscounting(datasetTrainList[iteration], datasetDevelopmentList[iteration])\n",
        "    print('Iteration: ',iteration+1)\n",
        "    print('Optimal paramater set: ',beta)\n",
        "    print('Perplexity Discounting: ', perplexityDiscounting)\n",
        "    print('\\n')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration:  1\n",
            "Optimal paramater set:  0.67\n",
            "Perplexity Discounting:  402.02378918743557\n",
            "\n",
            "\n",
            "Iteration:  2\n",
            "Optimal paramater set:  0.62\n",
            "Perplexity Discounting:  388.79809816818016\n",
            "\n",
            "\n",
            "Iteration:  3\n",
            "Optimal paramater set:  0.6\n",
            "Perplexity Discounting:  383.4649507845885\n",
            "\n",
            "\n",
            "Iteration:  4\n",
            "Optimal paramater set:  0.64\n",
            "Perplexity Discounting:  368.53634401389087\n",
            "\n",
            "\n",
            "Iteration:  5\n",
            "Optimal paramater set:  0.67\n",
            "Perplexity Discounting:  376.5628163719391\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l_Z1_SUITD2S"
      },
      "source": [
        "#### 1.4 Model's Performance on Test Set"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YyqGJkN2TJ2o"
      },
      "source": [
        "###### Optimal Parameters and Perplexity - *Interpolation*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r_PBqsBYTLki",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c1733fff-c69a-4b2a-d1b6-0fd3bf730d45"
      },
      "source": [
        "# finds the optimal parameter set and the corresponding perplexity on test set for interpolation trained on five different sets\n",
        "listOfperplexityInterpolation = []\n",
        "for iteration in range(0,5):\n",
        "    parameterSet, perplexityInterpolation = getLambdaAndPerplexityInterpolation(datasetTrainList[iteration], datasetTest)\n",
        "    listOfperplexityInterpolation.append(perplexityInterpolation)\n",
        "    print('Iteration: ',iteration+1)\n",
        "    print('Optimal paramater set: ',parameterSet)\n",
        "    print('Perplexity Interpolation: ', perplexityInterpolation)\n",
        "    print('\\n')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration:  1\n",
            "Optimal paramater set:  [0.3211033830089687, 0.4095029299449679, 0.2693936870460634]\n",
            "Perplexity Interpolation:  82.8701656214495\n",
            "\n",
            "\n",
            "Iteration:  2\n",
            "Optimal paramater set:  [0.3211017789074348, 0.409491066321625, 0.2694071547709402]\n",
            "Perplexity Interpolation:  82.85584296951804\n",
            "\n",
            "\n",
            "Iteration:  3\n",
            "Optimal paramater set:  [0.3211910403132751, 0.4094774173354341, 0.26933154235129086]\n",
            "Perplexity Interpolation:  82.87979338483134\n",
            "\n",
            "\n",
            "Iteration:  4\n",
            "Optimal paramater set:  [0.3211519344538609, 0.40959752882889033, 0.2692505367172487]\n",
            "Perplexity Interpolation:  82.91541109966278\n",
            "\n",
            "\n",
            "Iteration:  5\n",
            "Optimal paramater set:  [0.32098491142145213, 0.4094641674474203, 0.26955092113112755]\n",
            "Perplexity Interpolation:  82.82966927039946\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BHNgFR8xTPTc"
      },
      "source": [
        "###### Optimal Parameters and Perplexity - *Discounting*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JomDP1uVTQqg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2d67af88-5898-499a-b98b-8b82de7694bb"
      },
      "source": [
        "# finds the optimal beta and the corresponding perplexity on test set for discounting trained on five different sets\n",
        "listOfperplexityDiscounting = []\n",
        "for iteration in range(0,5):\n",
        "    beta, perplexityDiscounting = findOptimalBetaAndPerplexityDiscounting(datasetTrainList[iteration], datasetTest)\n",
        "    listOfperplexityDiscounting.append(perplexityDiscounting)\n",
        "    print('Iteration: ',iteration+1)\n",
        "    print('Optimal paramater set: ',beta)\n",
        "    print('Perplexity Discounting: ', perplexityDiscounting)\n",
        "    print('\\n')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration:  1\n",
            "Optimal paramater set:  0.67\n",
            "Perplexity Discounting:  372.22469249797877\n",
            "\n",
            "\n",
            "Iteration:  2\n",
            "Optimal paramater set:  0.65\n",
            "Perplexity Discounting:  371.58097868999346\n",
            "\n",
            "\n",
            "Iteration:  3\n",
            "Optimal paramater set:  0.69\n",
            "Perplexity Discounting:  377.72073464050703\n",
            "\n",
            "\n",
            "Iteration:  4\n",
            "Optimal paramater set:  0.68\n",
            "Perplexity Discounting:  376.5780896216441\n",
            "\n",
            "\n",
            "Iteration:  5\n",
            "Optimal paramater set:  0.67\n",
            "Perplexity Discounting:  374.9597414209729\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IleKYYMvUmJs"
      },
      "source": [
        "#### 1.5 Laplace Smoothing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C5_1t82Jn2RZ"
      },
      "source": [
        "###### Utility function for obtaining trigram laplace smoothed probabilities"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JNPo3rwgnsW-"
      },
      "source": [
        "# calculates the smoothed probabilities for trigrams after laplace smoothing\n",
        "def getLaplaceProbTrigramDict(freqOfTrigramDict, freqOfBigramDict):\n",
        "    K = 1\n",
        "    laplaceProbTrigramDict = {}\n",
        "    for trigram in freqOfTrigramDict:\n",
        "        bigram = (trigram[0], trigram[1])\n",
        "        laplaceProbTrigramDict[trigram] = (freqOfTrigramDict.get(trigram) + K) / (freqOfBigramDict.get(bigram) + K * len(freqOfTrigramDict))\n",
        "    return laplaceProbTrigramDict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-S4JhUpUn9Ia"
      },
      "source": [
        "###### Utility function for obtaining perplexity after laplace smoothing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R_WCCUfrUp9A"
      },
      "source": [
        "# calculates the perplexity after laplace smoothing\n",
        "def evaluatePerplexityLaplace(tokens, laplaceProbTrigramDict):\n",
        "    total = 0\n",
        "    # summation of logarithm of discounted probabilities\n",
        "    sumOfLogProb = 0\n",
        "    for j in range(len(tokens) - 2):\n",
        "        trigram = (tokens[j], tokens[j + 1], tokens[j + 2])\n",
        "        if trigram in laplaceProbTrigramDict:\n",
        "            prob = laplaceProbTrigramDict[trigram]\n",
        "        else:\n",
        "            prob = 0\n",
        "        if prob == 0:\n",
        "            sumOfLogProb += 0\n",
        "        else:\n",
        "            sumOfLogProb += math.log(prob, 2)\n",
        "        total += 1\n",
        "    # calculates the entropy\n",
        "    entropy = (-1 * sumOfLogProb ) / total\n",
        "    # perplexity is 2^entropy\n",
        "    perplexity = math.pow(2, entropy)\n",
        "    return perplexity"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kblw4ctrVSIE"
      },
      "source": [
        "# utility function for obtaining perplexity\n",
        "def getPerplexityLaplace(datasetTrain, datasetTest):\n",
        "    freqOfUnigramDict, freqOfBigramDict, freqOfTrigramDict, allBigramsList, allTrigramsList, probUnigramDict, probBigramDict, probTrigramDict = getAlldatasetTrainDetails(datasetTrain)\n",
        "    laplaceProbTrigramDict = getLaplaceProbTrigramDict(freqOfTrigramDict, freqOfBigramDict)\n",
        "    perplexity = evaluatePerplexityLaplace(datasetTest, laplaceProbTrigramDict)\n",
        "    return perplexity"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fdvgNIQBqQ36",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0178d630-7afb-4dd4-963f-f81111b82fe8"
      },
      "source": [
        "# finds the perplexity values on test set for laplace smoothing trained on five different sets\n",
        "listOfperplexityLaplace = []\n",
        "for iteration in range(0,5):\n",
        "    perplexityLaplace = getPerplexityLaplace(datasetTrainList[iteration], datasetTest)\n",
        "    listOfperplexityLaplace.append(perplexityLaplace)\n",
        "    print('Iteration: ',iteration+1)\n",
        "    print('Perplexity Laplace: ', perplexityLaplace)\n",
        "    print('\\n')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration:  1\n",
            "Perplexity Laplace:  668.4223883436812\n",
            "\n",
            "\n",
            "Iteration:  2\n",
            "Perplexity Laplace:  667.709718953416\n",
            "\n",
            "\n",
            "Iteration:  3\n",
            "Perplexity Laplace:  666.1954258895079\n",
            "\n",
            "\n",
            "Iteration:  4\n",
            "Perplexity Laplace:  666.4257546927955\n",
            "\n",
            "\n",
            "Iteration:  5\n",
            "Perplexity Laplace:  665.7075762986127\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sjnd-M2DcCqp"
      },
      "source": [
        "###### Variance of Perplexity Values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BChkQGc-woL4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8fac1ab9-5821-4ab6-f5cd-4f9bbc5bcabb"
      },
      "source": [
        "# obtain variance of perplexity values after interpolation, discounting and laplace methods\n",
        "print(\"Variance of perplexity laplace is % s\" %(statistics.variance(listOfperplexityLaplace)))\n",
        "print(\"Variance of perplexity interpolation is % s\" %(statistics.variance(listOfperplexityInterpolation)))\n",
        "print(\"Variance of perplexity discounting is % s\" %(statistics.variance(listOfperplexityDiscounting)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Variance of perplexity laplace is 1.2790530755490683\n",
            "Variance of perplexity interpolation is 0.00085600000020349\n",
            "Variance of perplexity discounting is 5.7136960000033\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mRouzBOFKkRU"
      },
      "source": [
        "### 2 - Vector Semantics:  GloVE implementation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vJO7rLbBuENt"
      },
      "source": [
        "#### 2.0 - PREREQUISITES"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xnaXjZ_huMBD"
      },
      "source": [
        "#### Install basic libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Xc0KBhmuV1F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "30e89c43-4bde-4c01-99c4-87e1f11fc30b"
      },
      "source": [
        "# Import required libraries\n",
        "! rm -rf web\n",
        "! cp -r '/content/drive/My Drive/Intelligent_Systems_CS565/Assignment-2/word-embeddings-benchmarks/web' .\n",
        "import numpy as np\n",
        "from math import log\n",
        "import pickle\n",
        "from tqdm import tqdm\n",
        "from collections import Counter\n",
        "from scipy import sparse\n",
        "from web.datasets.similarity import *\n",
        "from web.evaluate import evaluate_similarity\n",
        "from web.embeddings import fetch_GloVe"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.datasets.base module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.datasets. Anything that cannot be imported from sklearn.datasets is now part of the private API.\n",
            "  warnings.warn(message, FutureWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qkHp6ZVaufeI"
      },
      "source": [
        "#### 2.1 - GloVe embedding method implementation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tH96kiJFdVOF"
      },
      "source": [
        "###### Parameters used for glove embeddings implementation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "74OPvzS_dTV7"
      },
      "source": [
        "# declaring parameters used in glove embeddings implementation\n",
        "CONTEXT_WINDOW = 10\n",
        "DIM_SIZE = 100\n",
        "ITERATIONS = 45\n",
        "LEARNING_RATE = 0.05\n",
        "X_MAX = 100\n",
        "ALPHA = 0.75"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P9R-fdeScUjR"
      },
      "source": [
        "###### Utility function for constructing vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p9P7DPKpudhG"
      },
      "source": [
        "# Builds a vocabulary of words mapped to word ID and word frequency in the corpus\n",
        "def constructVocab(dataset):\n",
        "  vocab = Counter()\n",
        "  listOfSentences = nltk.sent_tokenize(dataset)\n",
        "  for sentence in listOfSentences:\n",
        "    listOfTokens = nltk.word_tokenize(sentence)\n",
        "    vocab.update(listOfTokens)\n",
        "  return {token: (tokenID, tokenFrequency) for tokenID, (token, tokenFrequency) in enumerate(vocab.items())}, listOfSentences"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wNQfwF8TcaaL"
      },
      "source": [
        "###### Utility function for constructing co-occurence matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GvLpsdOKupw9"
      },
      "source": [
        "#Builds the word Co-occurrence matrix X for the given vocabulary.\n",
        "# This is represented in a list of tuples containt {centerID, contextID, Xij}\n",
        "def constructCoOccurenceMatrix(vocab, listOfSentences, contextWindow):\n",
        "  lengthOfVocab = len(vocab)\n",
        "  output = []\n",
        "  #initialize a matrix of size V * V where V is the vocabSize\n",
        "  cooccurrences = sparse.lil_matrix((lengthOfVocab, lengthOfVocab), dtype=np.float64)\n",
        "  #Iterate over all the sentences in the corpus\n",
        "  for i, sentence in enumerate(listOfSentences):\n",
        "    listOfTokens = nltk.word_tokenize(sentence)\n",
        "    tokenIds = [vocab[token][0] for token in listOfTokens]\n",
        "\n",
        "    for i, centerId in enumerate(tokenIds):\n",
        "      contextIds = tokenIds[max(0, i - contextWindow) : i]\n",
        "      contextLen = len(contextIds)\n",
        "\n",
        "      #Iterate over each context word in left window\n",
        "      for j, contextId in enumerate(contextIds):\n",
        "        distance = contextLen - j\n",
        "        increment = 1.0 / float(distance)\n",
        "        #Update the matrix for contextWord and centerWord symmetrically to handle both left and right window co-occurences\n",
        "        cooccurrences[centerId, contextId] += increment\n",
        "        cooccurrences[contextId, centerId] += increment\n",
        "  \n",
        "  #Build the output in format of list of tuples from the sparse matrix\n",
        "  for i, (row, data) in enumerate(zip(cooccurrences.rows, cooccurrences.data)):\n",
        "    for dataIndex, j in enumerate(row):\n",
        "      output.append((i, j, data[dataIndex]))\n",
        "  \n",
        "  return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1bUTBNd_chkh"
      },
      "source": [
        "###### Utility function for performing a particular iteration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mmEuL5mVuqJZ"
      },
      "source": [
        "#Runs a single iteration of AdaGrad while training the GloVe embeddings.\n",
        "#Returns the cost associated with the given weights and updates the weights by AdaGrad logic\n",
        "def performIteration(vocab, data, rateOfLearning, xMax, alpha):\n",
        "  costGlobal = 0\n",
        "  random.shuffle(data)\n",
        "\n",
        "  for (vMain, vContext, bMain, bContext, gradWMain, gradWContext,\n",
        "       gradBMain, gradBContext, cooccurrence) in data:\n",
        "      \n",
        "    weight = (cooccurrence / xMax) ** alpha if cooccurrence < xMax else 1\n",
        "    innerCost = (vMain.dot(vContext) + bMain[0] + bContext[0] - log(cooccurrence))\n",
        "    cost = weight * (innerCost ** 2)\n",
        "    costGlobal += 0.5 * cost\n",
        "\n",
        "    #Compute gradients for word vectors and bias terms\n",
        "    mainGradient = weight * innerCost * vContext\n",
        "    contextGradient = weight * innerCost * vMain\n",
        "    mainGradientBias = weight * innerCost\n",
        "    contextGradientBias = weight * innerCost\n",
        "\n",
        "    #Perform adaptive gradient descent for word vectors\n",
        "    vMain -= (rateOfLearning * mainGradient / np.sqrt(gradWMain))\n",
        "    vContext -= (rateOfLearning * contextGradient / np.sqrt(gradWContext))\n",
        "    #Perform adaptive gradient descent for bias\n",
        "    bMain -= (rateOfLearning * mainGradientBias / np.sqrt(gradBMain))\n",
        "    bContext -= (rateOfLearning * contextGradientBias / np.sqrt(gradBContext))\n",
        "\n",
        "    #Update squared gradient sums\n",
        "    gradWMain += np.square(mainGradient)\n",
        "    gradWContext += np.square(contextGradient)\n",
        "    gradBMain += mainGradientBias ** 2\n",
        "    gradBContext += contextGradientBias ** 2\n",
        "\n",
        "  return costGlobal"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c2eF4fPPcz82"
      },
      "source": [
        "###### Utility function for training the custom glove model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jB8y1cjOu41N"
      },
      "source": [
        "#trains the glove MOdel with given vocabulary, coocurrence matrix, iterations and other parametrs.\n",
        "#Returns the computed word vector matrix W of size 2V * D\n",
        "def trainGloveModel(vocab, cooccurrences, numberOfDimensions, iterations, rateOfLearning, xMax, alpha):\n",
        "  \n",
        "  lengthOfVocab = len(vocab)\n",
        "  W = (np.random.rand(lengthOfVocab * 2, numberOfDimensions) - 0.5) / float(numberOfDimensions + 1)\n",
        "  biases = (np.random.rand(lengthOfVocab * 2) - 0.5) / float(numberOfDimensions + 1)\n",
        "  gradient = np.ones((lengthOfVocab * 2, numberOfDimensions), dtype=np.float64)\n",
        "  gradientBiases = np.ones(lengthOfVocab * 2, dtype=np.float64)\n",
        "\n",
        "  #Building the data\n",
        "  data = [ (W[iMain], W[iContext + lengthOfVocab],\n",
        "            biases[iMain : iMain + 1],\n",
        "            biases[iContext + lengthOfVocab : iContext + lengthOfVocab + 1],\n",
        "            gradient[iMain], gradient[iContext + lengthOfVocab],\n",
        "            gradientBiases[iMain : iMain + 1],\n",
        "            gradientBiases[iContext + lengthOfVocab : iContext + lengthOfVocab + 1],\n",
        "            cooccurrence )\n",
        "            for iMain, iContext, cooccurrence in cooccurrences]\n",
        "\n",
        "  #Training the word vector matrix for given number of iterations\n",
        "  for i in tqdm(range(iterations)):\n",
        "    cost = performIteration(vocab, data, rateOfLearning, xMax, alpha)\n",
        "    print('Iteration', i, '- Cost:', cost)\n",
        "\n",
        "  #Return the word vector matrix\n",
        "  return W"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QSjVcfpcc_oJ"
      },
      "source": [
        "###### Run entire workflow"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0BZKHHjwu8DM"
      },
      "source": [
        "# utility function for running the entire workflow\n",
        "def customGloveEmbeddings(dataset, contextWindow, numberOfDimensions, iterations, rateOfLearning, xMax, alpha):\n",
        "  vocab, listOfSentences = constructVocab(dataset)\n",
        "  cooccurrences = constructCoOccurenceMatrix(vocab, listOfSentences, contextWindow)\n",
        "  W = trainGloveModel(vocab, cooccurrences, numberOfDimensions, iterations, rateOfLearning, xMax, alpha)\n",
        "  return W, vocab"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DO14J-XCu_tg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9605c629-f990-4d93-a22e-a410f3281c53"
      },
      "source": [
        "W, vocab = customGloveEmbeddings(englishText, contextWindow=CONTEXT_WINDOW, numberOfDimensions=DIM_SIZE,\n",
        "                            iterations=ITERATIONS, rateOfLearning=LEARNING_RATE,\n",
        "                            xMax=X_MAX, alpha=ALPHA)\n",
        "\n",
        "print('vocab Size:', len(vocab))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  2%|▏         | 1/45 [03:42<2:42:51, 222.08s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 0 - Cost: 180729.55594677987\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  4%|▍         | 2/45 [07:26<2:39:45, 222.92s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 1 - Cost: 111847.27206446821\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  7%|▋         | 3/45 [11:09<2:35:53, 222.70s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 2 - Cost: 94524.33091618393\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  9%|▉         | 4/45 [14:50<2:31:56, 222.36s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 3 - Cost: 84154.30392824278\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 11%|█         | 5/45 [18:31<2:27:57, 221.93s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 4 - Cost: 77435.62181795119\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 13%|█▎        | 6/45 [22:14<2:24:21, 222.09s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 5 - Cost: 72680.74286223915\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 16%|█▌        | 7/45 [25:55<2:20:30, 221.84s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 6 - Cost: 68948.74436294862\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 18%|█▊        | 8/45 [29:38<2:16:57, 222.10s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 7 - Cost: 65874.47464446847\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 20%|██        | 9/45 [33:19<2:13:11, 221.98s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 8 - Cost: 63234.65714391176\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 22%|██▏       | 10/45 [37:01<2:09:26, 221.91s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 9 - Cost: 60930.255699962734\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 24%|██▍       | 11/45 [40:43<2:05:48, 222.02s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 10 - Cost: 58877.1495454983\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 27%|██▋       | 12/45 [44:24<2:01:55, 221.69s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 11 - Cost: 57021.60507602375\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 29%|██▉       | 13/45 [48:04<1:57:56, 221.15s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 12 - Cost: 55335.43780658976\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 31%|███       | 14/45 [51:45<1:54:10, 221.00s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 13 - Cost: 53772.00850925762\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 33%|███▎      | 15/45 [55:25<1:50:22, 220.75s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 14 - Cost: 52328.55982181119\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 36%|███▌      | 16/45 [59:05<1:46:37, 220.62s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 15 - Cost: 50980.757879367775\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 38%|███▊      | 17/45 [1:02:46<1:42:58, 220.66s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 16 - Cost: 49722.18088154132\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 40%|████      | 18/45 [1:06:26<1:39:10, 220.41s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 17 - Cost: 48534.66663403655\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 42%|████▏     | 19/45 [1:10:06<1:35:27, 220.31s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 18 - Cost: 47415.907675969516\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 44%|████▍     | 20/45 [1:13:47<1:31:51, 220.46s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 19 - Cost: 46362.22589298821\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 47%|████▋     | 21/45 [1:17:28<1:28:20, 220.85s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 20 - Cost: 45369.348989069375\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 49%|████▉     | 22/45 [1:21:10<1:24:47, 221.20s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 21 - Cost: 44416.89467276007\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 51%|█████     | 23/45 [1:24:54<1:21:18, 221.77s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 22 - Cost: 43517.01223825005\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 53%|█████▎    | 24/45 [1:28:34<1:17:29, 221.39s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 23 - Cost: 42655.81436785449\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 56%|█████▌    | 25/45 [1:32:15<1:13:43, 221.20s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 24 - Cost: 41839.863066770726\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 58%|█████▊    | 26/45 [1:35:56<1:10:05, 221.32s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 25 - Cost: 41050.06425568099\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 60%|██████    | 27/45 [1:39:39<1:06:31, 221.77s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 26 - Cost: 40298.65521430434\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 62%|██████▏   | 28/45 [1:43:19<1:02:42, 221.30s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 27 - Cost: 39580.5689906622\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 64%|██████▍   | 29/45 [1:47:01<59:03, 221.50s/it]  "
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 28 - Cost: 38886.758514176385\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 67%|██████▋   | 30/45 [1:50:44<55:25, 221.69s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 29 - Cost: 38220.615779203195\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 69%|██████▉   | 31/45 [1:54:24<51:36, 221.19s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 30 - Cost: 37581.9380514562\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 71%|███████   | 32/45 [1:58:04<47:51, 220.91s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 31 - Cost: 36968.8713072592\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 73%|███████▎  | 33/45 [2:01:43<44:04, 220.40s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 32 - Cost: 36378.7348518065\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 76%|███████▌  | 34/45 [2:05:23<40:23, 220.32s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 33 - Cost: 35809.56900908623\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 78%|███████▊  | 35/45 [2:09:04<36:43, 220.33s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 34 - Cost: 35260.259238944534\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 80%|████████  | 36/45 [2:12:44<33:02, 220.31s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 35 - Cost: 34735.121954253715\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 82%|████████▏ | 37/45 [2:16:25<29:23, 220.49s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 36 - Cost: 34223.173694411365\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 84%|████████▍ | 38/45 [2:20:06<25:46, 220.87s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 37 - Cost: 33730.70838501809\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 87%|████████▋ | 39/45 [2:23:49<22:08, 221.38s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 38 - Cost: 33256.95323396626\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 89%|████████▉ | 40/45 [2:27:29<18:25, 221.10s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 39 - Cost: 32798.55322656581\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 91%|█████████ | 41/45 [2:31:10<14:43, 220.99s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 40 - Cost: 32353.934069973217\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 93%|█████████▎| 42/45 [2:34:52<11:03, 221.32s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 41 - Cost: 31924.558827774337\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 96%|█████████▌| 43/45 [2:38:33<07:22, 221.22s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 42 - Cost: 31508.892343546322\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 98%|█████████▊| 44/45 [2:42:14<03:40, 220.96s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 43 - Cost: 31106.852705025543\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 45/45 [2:45:56<00:00, 221.26s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 44 - Cost: 30717.776303014896\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "vocab Size: 76825\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jPrt9FJgd8PC"
      },
      "source": [
        "###### Save custom glove embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7THQZ5lEvDiY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ea1bbf60-9301-4bec-d141-5555c8bb03d7"
      },
      "source": [
        "# Final GloVe word embedding is obtained by adding the center word embedding and context word embedding for each word\n",
        "print(W.shape)\n",
        "embeddings = {}\n",
        "lengthOfVocab = len(vocab)\n",
        "for word, (tokenID, _) in vocab.items():\n",
        "  embeddings[word] = W[tokenID]+W[tokenID+lengthOfVocab]\n",
        "\n",
        "# Save the embeddings in a pickle file\n",
        "with open('embeddings.pickle', 'wb') as handle:\n",
        "    pickle.dump(embeddings, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "# Save weights in a pickle file\n",
        "with open('weights.pickle', 'wb') as handle:\n",
        "    pickle.dump(W, handle, protocol=pickle.HIGHEST_PROTOCOL)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(153650, 100)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HUNG0SexvPIy"
      },
      "source": [
        "#### 2.2 - Comparing Word Similarity"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "07FCsHHReISR"
      },
      "source": [
        "###### Load custom glove embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CSePa0MCCL1V"
      },
      "source": [
        "# load embeddings implemented from the saved file\n",
        "with open('embeddings.pickle', 'rb') as handle:\n",
        "    embeddings = pickle.load(handle)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gE87cfgbv7uj"
      },
      "source": [
        "###### Sample test benchmarks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UHkYczMXCMtV"
      },
      "source": [
        "# fetching multiple test benchamrks available\n",
        "testBenchmarks = {\n",
        "    \"RG65\": fetch_RG65(),\n",
        "    \"MTurk\": fetch_MTurk(),\n",
        "    \"RW\": fetch_RW(),\n",
        "    \"WS353\": fetch_WS353(),\n",
        "    \"MEN\": fetch_MEN(),\n",
        "    \"SIMLEX999\": fetch_SimLex999(),\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b4S3PvSZCNHA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "68a1494a-bdf5-408a-f662-bcc03ffa7208"
      },
      "source": [
        "# print some sample test cases of each benchmark\n",
        "for name, data in testBenchmarks.items():\n",
        "    print(\"Sample data from {}: pair \\\"{}\\\" and \\\"{}\\\" is assigned score {}\".format(name, data.X[0][0], data.X[0][1], data.y[0]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sample data from RG65: pair \"gem\" and \"jewel\" is assigned score 9.85\n",
            "Sample data from MTurk: pair \"episcopal\" and \"russia\" is assigned score 5.5\n",
            "Sample data from RW: pair \"squishing\" and \"squirt\" is assigned score 5.88\n",
            "Sample data from WS353: pair \"love\" and \"sex\" is assigned score 6.77\n",
            "Sample data from MEN: pair \"sun\" and \"sunlight\" is assigned score [10.]\n",
            "Sample data from SIMLEX999: pair \"old\" and \"new\" is assigned score 1.58\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XMi99vgswNHr"
      },
      "source": [
        "###### Performance evaluation on custom glove embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "24dLztBECNm_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0c5302f5-89f2-4182-8060-edb9047d47b2"
      },
      "source": [
        "# calls inbuilt evaluate similarity function to evaluate performance of glove embeddings implemented\n",
        "for name, data in testBenchmarks.items():\n",
        "    print(\"Spearman correlation of scores on {} {}\".format(name, evaluate_similarity(embeddings, data.X, data.y)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Missing 30 words. Will replace them with mean vector\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Spearman correlation of scores on RG65 0.1261666922886971\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Missing 125 words. Will replace them with mean vector\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Spearman correlation of scores on MTurk 0.17142783362215339\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Missing 1743 words. Will replace them with mean vector\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Spearman correlation of scores on RW 0.20722669678463418\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Missing 52 words. Will replace them with mean vector\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Spearman correlation of scores on WS353 0.19001707210564006\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Missing 803 words. Will replace them with mean vector\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Spearman correlation of scores on MEN 0.12444210895087617\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Missing 94 words. Will replace them with mean vector\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Spearman correlation of scores on SIMLEX999 0.06878120834230247\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sHjKRVhMwibz"
      },
      "source": [
        "###### Load pre-trained glove embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VdEMUyZnCODf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3dccffd0-422e-455e-ef94-fd5a128379d0"
      },
      "source": [
        "# Fetch pre trained glove embeddings\n",
        "gloveEmbeddings = fetch_GloVe(corpus=\"twitter-27B\", dim=100)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "File already downloaded, skipping\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oUgBumAMwcod"
      },
      "source": [
        "###### Performance evaluation on pre-trained embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mm4r9zmtCOkj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "77256b32-aa34-4e61-e6d6-aff8d3a0a08a"
      },
      "source": [
        "# calls inbuilt evaluate similarity function to evaluate performance of pre-trained embeddings\n",
        "for name, data in testBenchmarks.items():\n",
        "    print(\"Spearman correlation of scores on {} {}\".format(name, evaluate_similarity(gloveEmbeddings, data.X, data.y)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Missing 1 words. Will replace them with mean vector\n",
            "Missing 1071 words. Will replace them with mean vector\n",
            "Missing 25 words. Will replace them with mean vector\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Spearman correlation of scores on RG65 0.6774486160113895\n",
            "Spearman correlation of scores on MTurk 0.5641004632190647\n",
            "Spearman correlation of scores on RW 0.23074174522387267\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Missing 1 words. Will replace them with mean vector\n",
            "Missing 1 words. Will replace them with mean vector\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Spearman correlation of scores on WS353 0.46979381939437287\n",
            "Spearman correlation of scores on MEN 0.5773369776281995\n",
            "Spearman correlation of scores on SIMLEX999 0.12221121100798378\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}